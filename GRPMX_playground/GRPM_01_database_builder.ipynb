{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nbib\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pyperclip\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from Bio import Entrez\n",
    "Entrez.email = \"your_email@example.com\"\n",
    "\n",
    "request_counter = 0\n",
    "gene_counter = 0\n",
    "\n",
    "if not os.path.exists('grpm_db'):\n",
    "    os.makedirs('grpm_db')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## job forecast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Statistiche basate su 150 random genes:\n",
    "time_sleep = 0.4\n",
    "runtime_gene = 6.36 #sec/gene\n",
    "geni_ora = 566 #geni/ora\n",
    "request_counter_gene = 4.25 #request/gene (with base sleep (0.4))\n",
    "sleep_request_base = 0.4\n",
    "sleep_request_overnight_plus = 1.1 #plus/gene\n",
    "\n",
    "print('Previsioni:')\n",
    "max_genes = int(10000/request_counter_gene)\n",
    "print('max genes: ',max_genes)\n",
    "table_size_db_gene = 0.496 #MB\n",
    "table_size_gene = 0.397 #MB\n",
    "png_size_db_gene = 0.47 #KB\n",
    "\n",
    "#Previsioni:\n",
    "genes = pd.read_csv('human_genes_repo/H_GENES_proteincoding_genes.csv')\n",
    "ngenes = len(genes)#gene_range\n",
    "nruntime = ngenes * runtime_gene\n",
    "#print('runtime, '+str(ngenes), nruntime)\n",
    "nrequest_counter = ngenes * request_counter_gene\n",
    "print('request_counter, '+str(int(ngenes)),':', nrequest_counter)\n",
    "\n",
    "tempo_ore = round(nruntime/3600, 2)\n",
    "tempo_ore_overnight = round((nruntime+(sleep_request_overnight_plus*ngenes))/3600, 2)\n",
    "print('runtime_hours, '+str(int(ngenes)),':', tempo_ore)\n",
    "print('runtime_hours_overnight, '+str(int(ngenes)),':', tempo_ore_overnight)\n",
    "\n",
    "db_table_size = ngenes * table_size_gene\n",
    "print('db_table_size_MB, '+str(int(ngenes)),':', round(db_table_size,2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LOG: with na:\n",
    "    protein_coding_genes 19383\n",
    "    IG_TR_genes 660\n",
    "    RNA_genes 14691\n",
    "    pseudo_genes 13426\n",
    "    misc_genes 55\n",
    "\n",
    "dropna():\n",
    "    protein_coding_genes 19318\n",
    "    IG_TR_genes 641\n",
    "    RNA_genes 11452\n",
    "    pseudo_genes 9866\n",
    "    misc_genes 22"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.howto-connect.com/automatically-hibernate-windows-10-scheduled-time/\n",
    "Write the number of seconds replacing xx after which you want to hibernate your system in the below command â€“\n",
    "    timeout /t xx /NOBREAK > NUL && shutdown /h\n",
    "\n",
    "time start 17:00 --> time hybernation 00:01\n",
    "    timeout /t 35000 /NOBREAK > NUL && shutdown /h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load Human Genes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Human Gene list\n",
    "protein_coding_genes = pd.read_csv('human_genes_repo/H_GENES_proteincoding_genes.csv')\n",
    "IG_TR_genes          = pd.read_csv('human_genes_repo/H_GENES_IGTR_genes.csv')\n",
    "RNA_genes            = pd.read_csv('human_genes_repo/H_GENES_RNA_genes.csv')\n",
    "pseudo_genes         = pd.read_csv('human_genes_repo/H_GENES_pseudo_genes.csv')\n",
    "misc_genes           = pd.read_csv('human_genes_repo/H_GENES_misc_genes.csv')\n",
    "\n",
    "#First: protein coding genes:\n",
    "protein_coding_genes_list = protein_coding_genes['Gene name'].dropna().tolist()\n",
    "RNA_genes_list = RNA_genes['Gene name'].dropna().tolist()\n",
    "pseudo_genes_list = pseudo_genes['Gene name'].dropna().tolist()\n",
    "\n",
    "#set source gene list\n",
    "surce_gene_list = protein_coding_genes_list\n",
    "\n",
    "#protein_coding_genes_list = list(map(str, protein_coding_genes_list))\n",
    "#set interval\n",
    "gene_range = int(len(surce_gene_list)/18)\n",
    "gene_range = 2300\n",
    "genes01 = surce_gene_list[:gene_range]\n",
    "genes02 = surce_gene_list[gene_range:gene_range*2]\n",
    "genes03 = surce_gene_list[gene_range*2:gene_range*3]\n",
    "genes04 = surce_gene_list[gene_range*3:gene_range*4]\n",
    "genes05 = surce_gene_list[gene_range*4:gene_range*5]\n",
    "genes06 = surce_gene_list[gene_range*5:gene_range*6]\n",
    "genes07 = surce_gene_list[gene_range*6:gene_range*7]\n",
    "genes08 = surce_gene_list[gene_range*7:gene_range*8]\n",
    "genes09 = surce_gene_list[gene_range*8:gene_range*9]\n",
    "slicer = 1150\n",
    "genes02_01 = genes02[:slicer]\n",
    "genes02_02 = genes02[ slicer:]\n",
    "genes03_01 = genes03[:slicer]\n",
    "genes03_02 = genes03[ slicer:]\n",
    "genes04_01 = genes04[:slicer]\n",
    "genes04_02 = genes04[ slicer:]\n",
    "genes05_01 = genes05[:slicer]\n",
    "genes05_02 = genes05[ slicer:]\n",
    "genes06_01 = genes06[:slicer]\n",
    "genes06_02 = genes06[ slicer:]\n",
    "genes07_01 = genes07[:slicer]\n",
    "genes07_02 = genes07[ slicer:]\n",
    "genes08_01 = genes08[:slicer]\n",
    "genes08_02 = genes08[ slicer:]\n",
    "genes09_01 = genes09[:slicer]\n",
    "\n",
    "#Second: RNA genes:\n",
    "rna_genes_list = RNA_genes['Gene name'].dropna().tolist()\n",
    "rnagenes01 = rna_genes_list[:gene_range]\n",
    "rnagenes02 = rna_genes_list[gene_range:gene_range*2]\n",
    "rnagenes02_01 = rnagenes02[:1150]\n",
    "rnagenes02_02 = rnagenes02[1150:]\n",
    "#genes1, genes2\n",
    "#protein_coding_genes[:20]\n",
    "pyperclip.copy(str(rnagenes02_01))\n",
    "\n",
    "print('protein_coding_genes',len(protein_coding_genes['Gene name'].dropna()),\n",
    "      '\\nIG_TR_genes',len(IG_TR_genes['Gene name'].dropna()),\n",
    "      '\\nRNA_genes',len(RNA_genes['Gene name'].dropna()),\n",
    "      '\\npseudo_genes',len(pseudo_genes['Gene name'].dropna()),\n",
    "      '\\nmisc_genes',len(misc_genes['Gene name'].dropna()))\n",
    "\n",
    "len(rnagenes02_01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "stack = 1150 + 0\n",
    "#genes04_01 = genes04[111:1150]\n",
    "pyperclip.copy(str(genes04_01))\n",
    "len(genes04_01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## set parameters\n",
    "## import datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set parameters:--------------------------------\n",
    "save_studytype_data = False\n",
    "save_plot           = False\n",
    "save_accessory_data = False\n",
    "db_tag = 'trial'\n",
    "db_name = 'grpm_db'+'_'+db_tag\n",
    "#------------------------------------------------\n",
    "\n",
    "if not os.path.exists(db_name):\n",
    "    os.makedirs(db_name)\n",
    "\n",
    "# create/import datasheet\n",
    "time_a = datetime.now()\n",
    "if os.path.isfile(db_name+'/grpm_table_output.csv'):\n",
    "    complete_df = pd.read_csv(db_name+'/grpm_table_output.csv', index_col=0)\n",
    "else:\n",
    "    complete_df = pd.DataFrame()\n",
    "\n",
    "if os.path.isfile(db_name+'/complete_nbibtable.csv'):\n",
    "    complete_nbibtable = pd.read_csv(db_name+'/complete_nbibtable.csv', index_col=0)\n",
    "else:\n",
    "    complete_nbibtable = pd.DataFrame()\n",
    "time_b = datetime.now()\n",
    "\n",
    "print('time load',time_b-time_a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check data\n",
    "if os.path.isfile(db_name+'/grpm_table_output.csv'):\n",
    "    gene_db_count =  complete_df.gene.nunique()\n",
    "    print('complete_df gene count:',gene_db_count,'on', len(surce_gene_list))\n",
    "    if gene_db_count >= 15519:\n",
    "        print('grpm db already contains all available genes on litvar1')\n",
    "\n",
    "    print('\\ngrpm_table_output.csv size'  ,round(os.path.getsize(db_name+'/grpm_table_output.csv')/(1024*1024),3),'MB')\n",
    "    print('complete_nbibtable.csv size',round(os.path.getsize(db_name+'/complete_nbibtable.csv')/(1024*1024),3),'MB')\n",
    "    print('memory_usage_complete_df'     ,round(complete_df.memory_usage().sum()/(1024*1024),3))\n",
    "    print('memory_usage_complete_nbib_df',round(complete_nbibtable.memory_usage().sum()/(1024*1024),3))\n",
    "else:\n",
    "    print('empty database')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Job"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get modulule\n",
    "\n",
    "# Set gene range\n",
    "genes = pd.Series(genes01).sample(5)\n",
    "#when stucked use this:\n",
    "skipped_genes =  []\n",
    "#genes = ['APOA1', 'FFC1', 'ERH', 'USP53']\n",
    "pyperclip.copy(str(genes))\n",
    "\n",
    "time_start = datetime.now()\n",
    "print('Start at ',time_start)\n",
    "\n",
    "# get gene data\n",
    "for i in genes:\n",
    "    #LitVar2 \"Variants for Gene\" API request\n",
    "    if request_counter > 9950:\n",
    "        #print('Request limit reached. Wait \\'till tomorrow!')\n",
    "        #break\n",
    "        pass\n",
    "    gene = i\n",
    "    time_alpha = datetime.now()\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/research/litvar2-api/variant/search/gene/\" + gene\n",
    "    foo = (rq.get(url)).text\n",
    "\n",
    "    # parsing output in JSON\n",
    "    foo = foo.replace(\"\\n\",\", \")\n",
    "    foo = foo.replace(\"\\'\",\"\\\"\")\n",
    "    foo = foo.replace('\\\"\\\"', '\\\"')\n",
    "    foo = foo.replace('p.\\\"','p.')\n",
    "    foo = foo.replace('c.\\\"','c.')\n",
    "    foo = foo.replace('g.\\\"','g.')\n",
    "    foo = foo.replace('\\\">','>')\n",
    "    foo = foo.replace('.C\\\"204','.C204')\n",
    "\n",
    "\n",
    "\n",
    "    data=\"[\" + foo + \"]\"\n",
    "\n",
    "    #Create Dataframe\n",
    "    df = pd.read_json(data)\n",
    "    if 'rsid' in df.columns and len(df.rsid)>1:\n",
    "        # creare un df senza i clingen.\n",
    "        dfb = df[['_id','pmids_count','rsid']]\n",
    "        dfa = dfb[~dfb['_id'].str.contains('@CA')].drop_duplicates().reset_index(drop=True)\n",
    "        dfn = dfa.dropna(subset=['rsid'])\n",
    "\n",
    "        #Statistics\n",
    "        handle = Entrez.esearch(db=\"snp\", term=gene)\n",
    "        record = Entrez.read(handle)\n",
    "        request_counter += 1\n",
    "\n",
    "        NCBI_dbSNP = record[\"Count\"]\n",
    "        lit2_variant = len(dfa['_id'].drop_duplicates())\n",
    "        lit2_variant_norsid = len(dfa.loc[df['rsid'].isna()])\n",
    "        lit2_rsid = len(dfn.rsid.drop_duplicates())\n",
    "\n",
    "        #^print('Statistics, LitVar2:')\n",
    "        #^print(\"Gene:\", gene)\n",
    "        #^print('NCBI dbSNP:', NCBI_dbSNP, 'rsid for', gene,'gene') #search gnomad api\n",
    "        #^print(\"variant_count:\", lit2_variant)\n",
    "        #^print('variant without rsid:', lit2_variant_norsid)\n",
    "        #^print(\"rsid_count:\", lit2_rsid)#, 'diff:', int(record[\"Count\"])-len(dfn))\n",
    "\n",
    "        # remove rs with pmid_count=1\n",
    "        df2 = dfn.loc[df.pmids_count !=1]#.reset_index(drop=True)\n",
    "        lit2_rsid_f = len(df2)\n",
    "\n",
    "        #^print(\"rsid_count(pmid>1):\", lit2_rsid_f)#, 'diff:', len(dfn)-len(df2)\n",
    "        dfsort = df.sort_values(by='pmids_count',ascending=False).reset_index(drop=True)\n",
    "        df2sort = df2.sort_values(by='pmids_count',ascending=False).reset_index(drop=True)\n",
    "        #^display(df2sort[[\"rsid\",\"pmids_count\"]].head(15))\n",
    "\n",
    "\n",
    "        #LitVar1 POST rsid2pmids\n",
    "        # convert in \"data\" for POST request\n",
    "        liss = list(map(str, df.rsid))\n",
    "        qrsid = \"\\\", \\\"\".join(liss)\n",
    "        qrsid = '{\"rsids\": [\"'+ qrsid +'\"]}'\n",
    "\n",
    "        url=\"https://www.ncbi.nlm.nih.gov/research/bionlp/litvar/api/v1/public/rsids2pmids\"\n",
    "        headers={ \"Content-Type\" : \"application/json\", \"Accept\" : \"application/json\"}\n",
    "\n",
    "        # inserire i miei data=rsID list\n",
    "        data = qrsid\n",
    "        #data = {[\"rs781028867\", \"rs17817449\"]}\n",
    "        r = rq.post(url, headers=headers, data=data )\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "            sys.exit()\n",
    "        rspost = r.json()\n",
    "\n",
    "        #Display\n",
    "        dfrspost = pd.DataFrame(rspost)\n",
    "        if 'rsid' in dfrspost.columns and len(dfrspost.rsid)>1:\n",
    "            lit1_rsid = len(dfrspost.rsid)\n",
    "            #lit2_rsid = len(df)\n",
    "\n",
    "            #^print('Statistics, LitVar1:')\n",
    "            #^print('LitVar1 rsid:', lit1_rsid)\n",
    "            #^print('LitVar2 rsid:',lit2_rsid_f,'(diff:', len(df2)-len(dfrspost.rsid),')')\n",
    "            lit1_raw_pmid = 0\n",
    "            for i in range(len(dfrspost)):\n",
    "                lit1_raw_pmid += len(dfrspost.pmids[i])\n",
    "            #^print('raw pmid_count:', lit1_raw_pmid)\n",
    "            #^display(dfrspost[[\"rsid\",\"pmids\"]].head(10))\n",
    "\n",
    "\n",
    "            #Creare la lista semplice [rsid-pmid]\n",
    "            #[MODULO: \"Alligner\"]\n",
    "            rspmid = []\n",
    "            for i in range(len(dfrspost)):\n",
    "                for pmid in dfrspost['pmids'][i]: #dfrspost = mother table\n",
    "                    out = dfrspost['rsid'][i], pmid\n",
    "                    rspmid.append(out)\n",
    "\n",
    "            rsidpmid = pd.DataFrame(rspmid).drop_duplicates().rename(columns={0: 'rsid',1:'pmids'})\n",
    "            rsidpmid['pmids'] = rsidpmid['pmids'].astype(str) #convert pmid type in str\n",
    "            #report data:\n",
    "            lit1_rsid_pmid = len(rsidpmid)\n",
    "            lit1_pmid = len(rsidpmid.drop_duplicates(subset='pmids'))\n",
    "            #^print(\"total [rsid-pmid]:\",len(rsidpmid))\n",
    "            #^print('rsid_count:', lit1_rsid)\n",
    "            #^print('pmid_count:',lit1_pmid)\n",
    "            #print('\\n',rsidpmid.head(10))\n",
    "\n",
    "            ####[MODULO: groupby.describe]\n",
    "            # applicare groupby ad rsidpmid per avere tabella pmid count\n",
    "            rsidpmidcount = rsidpmid.groupby('rsid').describe().reset_index()\n",
    "            rsidpmidcount.columns = rsidpmidcount.columns.to_flat_index()\n",
    "            #replace column names\n",
    "            new_column_names = ['rsid', 'pmid_count', 'pmid_unique','pmid_top','pmid_freq']\n",
    "            rsidpmidcount.columns = new_column_names\n",
    "            rsidpmidcountf = rsidpmidcount[['rsid','pmid_unique']]\n",
    "            #report data:\n",
    "            lit1_rsid_f = len(rsidpmidcountf[rsidpmidcountf.pmid_unique!=1])\n",
    "            lit1_rsid_m = len(rsidpmidcountf[rsidpmidcountf.pmid_unique==1])\n",
    "            #^print ('\\nrsid (pmid>1):',lit1_rsid_f)\n",
    "            #^print ('rsid (pmid=1)',lit1_rsid_m)\n",
    "\n",
    "            rsidpmidcountfsort = rsidpmidcountf.sort_values('pmid_unique',ascending=False).reset_index(drop=True)\n",
    "            #^print('\\n',rsidpmidcountfsort.head(10))\n",
    "\n",
    "            #Filter pmid for rsid with pmid>1\n",
    "            outless = rsidpmidcountfsort[rsidpmidcountfsort.pmid_unique>1]\n",
    "            #creare una mask isin su rsidpmid con outless.rsid\n",
    "            mask = rsidpmid['rsid'].isin(outless.rsid)\n",
    "            rsidpmidless = rsidpmid[mask]\n",
    "            lit1_pmid_f = len(rsidpmidless.pmids.drop_duplicates())\n",
    "            #^print('\\nLitVar1 results:')\n",
    "            #^print('total rsid:', lit1_rsid)\n",
    "            #^print('filtered rsid (pmid>1):',lit1_rsid_f)\n",
    "            #^print('total pmids:', lit1_pmid)\n",
    "            #^print('filtered pmids:',lit1_pmid_f)\n",
    "            #rsidpmidless\n",
    "\n",
    "            # PubMed query Build:\n",
    "            ### two input alternatives (total LitVar1 and LitVar>1)\n",
    "            #Total\n",
    "            pmdq1 = rsidpmid.pmids.drop_duplicates().tolist()\n",
    "            #^print(gene,'pmids:',len(pmdq1))\n",
    "\n",
    "            #pmid>1\n",
    "            pmdq2 = rsidpmidless.pmids.drop_duplicates().tolist()\n",
    "            pmdq2 = list(map(str, pmdq2))\n",
    "            #^print(gene,'pmids (n>1):',len(pmdq2))\n",
    "\n",
    "            #Query chunk build (max:1300)\n",
    "            ##https://www.entechin.com/python-split-list/\n",
    "            limit = 1300\n",
    "\n",
    "            if len(pmdq1)<=limit:\n",
    "                wkl01 = pmdq1\n",
    "\n",
    "            if limit<len(pmdq1)<=limit*2:\n",
    "                j = len(pmdq1)//2\n",
    "                wkl01=pmdq1[:j]\n",
    "                wkl02=pmdq1[j:]\n",
    "\n",
    "            if limit*2<len(pmdq1)<=limit*3:\n",
    "                j = len(pmdq1)//3\n",
    "                wkl01=pmdq1[:j]\n",
    "                wkl02=pmdq1[j:j*2]\n",
    "                wkl03=pmdq1[j*2:]\n",
    "\n",
    "            if limit*3<len(pmdq1)<=limit*4:\n",
    "                j = len(pmdq1)//4\n",
    "                wkl01=pmdq1[:j]\n",
    "                wkl02=pmdq1[j:j*2]\n",
    "                wkl03=pmdq1[j*2:j*3]\n",
    "                wkl04=pmdq1[j*3:]\n",
    "\n",
    "            if limit*4<len(pmdq1)<=limit*5:\n",
    "                j = len(pmdq1)//5\n",
    "                wkl01=pmdq1[:j]\n",
    "                wkl02=pmdq1[j:j*2]\n",
    "                wkl03=pmdq1[j*2:j*3]\n",
    "                wkl04=pmdq1[j*3:j*4]\n",
    "                wkl05=pmdq1[j*4:]\n",
    "\n",
    "            if limit*5<len(pmdq1)<=limit*6:\n",
    "                j = len(pmdq1)//6\n",
    "                wkl01=pmdq1[:j]\n",
    "                wkl02=pmdq1[j:j*2]\n",
    "                wkl03=pmdq1[j*2:j*3]\n",
    "                wkl04=pmdq1[j*3:j*4]\n",
    "                wkl05=pmdq1[j*4:j*5]\n",
    "                wkl06=pmdq1[j*5:]\n",
    "\n",
    "            if limit*6<len(pmdq1)<=limit*7:\n",
    "                j = len(pmdq1)//7\n",
    "                wkl01=pmdq1[:j]\n",
    "                wkl02=pmdq1[j:j*2]\n",
    "                wkl03=pmdq1[j*2:j*3]\n",
    "                wkl04=pmdq1[j*3:j*4]\n",
    "                wkl05=pmdq1[j*4:j*5]\n",
    "                wkl06=pmdq1[j*5:j*6]\n",
    "                wkl07=pmdq1[j*6:]\n",
    "\n",
    "            #if len(pmdq1)>limit*7:\n",
    "            #print('pmid query out of range')\n",
    "\n",
    "            #^print('\\npmid query1:',len(wkl01))\n",
    "            #if limit<len(pmdq1)<limit*2:\n",
    "            #print('pmid query2:',len(wkl02))\n",
    "            #print('sum:',len(wkl01+wkl02))\n",
    "            #if limit*2<len(pmdq1)<limit*3:\n",
    "            #print('pmid query2:',len(wkl02))\n",
    "            #print('pmid query3:',len(wkl03))\n",
    "            #print('sum:',len(wkl01+wkl02+wkl03))\n",
    "\n",
    "            ##Define list of queries for PubMed:\n",
    "            query = []\n",
    "\n",
    "            if len(pmdq1)<limit:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query = [query01]\n",
    "\n",
    "            if limit<len(pmdq1)<limit*2:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query02 = \"+OR+\".join(wkl02)\n",
    "                #print('chunks2:',len(wkl01+wkl02))\n",
    "                query = [query01, query02]\n",
    "\n",
    "            if limit*2<len(pmdq1)<limit*3:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query02 = \"+OR+\".join(wkl02)\n",
    "                query03 = \"+OR+\".join(wkl03)\n",
    "                #print('chunks3:',len(wkl01+wkl02+wkl03))\n",
    "                query = [query01, query02, query03]\n",
    "\n",
    "            if limit*3<len(pmdq1)<limit*4:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query02 = \"+OR+\".join(wkl02)\n",
    "                query03 = \"+OR+\".join(wkl03)\n",
    "                query04 = \"+OR+\".join(wkl04)\n",
    "                query = [query01, query02, query03, query04]\n",
    "\n",
    "            if limit*4<len(pmdq1)<limit*5:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query02 = \"+OR+\".join(wkl02)\n",
    "                query03 = \"+OR+\".join(wkl03)\n",
    "                query04 = \"+OR+\".join(wkl04)\n",
    "                query05 = \"+OR+\".join(wkl05)\n",
    "                query = [query01, query02, query03, query04, query05]\n",
    "\n",
    "            if limit*5<len(pmdq1)<limit*6:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query02 = \"+OR+\".join(wkl02)\n",
    "                query03 = \"+OR+\".join(wkl03)\n",
    "                query04 = \"+OR+\".join(wkl04)\n",
    "                query05 = \"+OR+\".join(wkl05)\n",
    "                query06 = \"+OR+\".join(wkl06)\n",
    "                query = [query01, query02, query03, query04, query05,query06]\n",
    "\n",
    "            if limit*6<len(pmdq1)<limit*7:\n",
    "                query01 = \"+OR+\".join(wkl01)\n",
    "                query02 = \"+OR+\".join(wkl02)\n",
    "                query03 = \"+OR+\".join(wkl03)\n",
    "                query04 = \"+OR+\".join(wkl04)\n",
    "                query05 = \"+OR+\".join(wkl05)\n",
    "                query06 = \"+OR+\".join(wkl06)\n",
    "                query07 = \"+OR+\".join(wkl07)\n",
    "                query = [query01, query02, query03, query04, query05,query06, query07]\n",
    "\n",
    "            #print(type(query), len(query))\n",
    "            #^print('pages:', (len(wkl01)//200)+1)\n",
    "\n",
    "            # Concatenzaione delle request per le due query\n",
    "            ### carefull: high runtime\n",
    "            #^print('nbib report:', gene)\n",
    "            #^print('start  at:', datetime.now())\n",
    "            time1 = datetime.now()\n",
    "            pages = ((len(wkl01)//200)+1)+1\n",
    "            if len(wkl01) % 200 == 0:\n",
    "                pages = pages -1\n",
    "            fullnbib = str()\n",
    "            for d in query:\n",
    "                for i in range(1,pages):\n",
    "                    page = str(i)\n",
    "                    url = 'https://pubmed.ncbi.nlm.nih.gov/?term=' + d + '&format=pubmed&size=200&page='+ page\n",
    "                    output = rq.get(url)\n",
    "                    html = output.text\n",
    "                    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "                    for script in soup([\"script\", \"style\"]):\n",
    "                        script.extract()\n",
    "                    text = soup.get_text()\n",
    "                    postString = text.split(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",2)[2]\n",
    "                    nbib01 = postString.replace('\\n\\n','')\n",
    "                    fullnbib += nbib01\n",
    "                    request_counter += pages\n",
    "                    time.sleep(1.5)\n",
    "                #if pages>6:\n",
    "                #print(gene+' slept for 1 sec every pubmed query')\n",
    "                #time.sleep(1)\n",
    "                #else:\n",
    "                #    pass\n",
    "            time2 = datetime.now()\n",
    "            timestamp = time2.strftime('%Y%m%d%H%M%S')\n",
    "            #^print('finish at:', datetime.now())\n",
    "            runtime = time2-time1\n",
    "            duration = str(runtime).split('.')[0]\n",
    "            hours, minutes, seconds = duration.split(':')\n",
    "            compact_duration = '{}:{}:{}'.format(hours, minutes, seconds)\n",
    "            #^print('runtime', duration)\n",
    "            #print(fullnbib)\n",
    "\n",
    "            # nbib parsing:\n",
    "            timea = datetime.now()\n",
    "            ref = nbib.read(fullnbib)\n",
    "            dfbib = pd.DataFrame(ref)\n",
    "            if 'descriptors' in dfbib.columns and len(dfbib['descriptors'])>1:\n",
    "                dfbibdes = dfbib[['pubmed_id','descriptors']].dropna().reset_index(drop=True)\n",
    "                nbib_objects = len(dfbib)\n",
    "                nbib_objects_withdescriptors = len(dfbibdes)\n",
    "                #^print('nibib objects:',nbib_objects)\n",
    "                #^print('nibib objects with descriptors:',len(dfbibdes))\n",
    "                timeb = datetime.now()\n",
    "                #^print('runtime:', timeb-timea)\n",
    "\n",
    "                #INFO: all nbib qualifier;\n",
    "                #dfbib.loc[1]\n",
    "\n",
    "                #Statistics:\n",
    "                pubmed_pmid_query = len(pmdq1)\n",
    "                pubmed_pmid_nbib = len(dfbib.pubmed_id.drop_duplicates())\n",
    "                pubmed_pmid_nbib_yesmesh = len(dfbibdes.pubmed_id.drop_duplicates())\n",
    "                pubmed_pmid_nbib_nomesh = len(dfbib.pubmed_id.drop_duplicates())-len(dfbibdes.pubmed_id.drop_duplicates())\n",
    "\n",
    "                #^print('\\nPubMed scraping:', gene, time1)\n",
    "                #^print('pmid in query:',pubmed_pmid_query)\n",
    "                #^print(f\"pmid in nbib: {pubmed_pmid_nbib}\")\n",
    "                #^print('   pmid with mesh:', pubmed_pmid_nbib_yesmesh)\n",
    "                #^print('   pmid without mesh:', pubmed_pmid_nbib_nomesh)\n",
    "                #print('   rsid for pmid with-mesh:', 'vedi dopo')\n",
    "                #display(dfbibdes.head(5))\n",
    "\n",
    "                dfr = []\n",
    "                for i in range(len(dfbibdes)):\n",
    "                    for mesh in dfbibdes['descriptors'][i]:\n",
    "                        out = dfbibdes['pubmed_id'][i], mesh\n",
    "                        dfr.append(out)\n",
    "                MESH = pd.DataFrame(dfr).rename(columns={0: 'pmids',1:'mesh'})\n",
    "\n",
    "                # dataframe parsing splitting three fields\n",
    "                MESHsplit =[]\n",
    "                for i in range(len(MESH)):\n",
    "                    #mg = dict(MESH2.mesh[i]).get('descriptor')\n",
    "                    mg = MESH.mesh[i].get('descriptor')\n",
    "                    mg2 = MESH.mesh[i].get('qualifier')\n",
    "                    mg3 = MESH.mesh[i].get('major')\n",
    "                    mgg = MESH.pmids[i], mg, mg2, mg3\n",
    "                    MESHsplit.append(mgg)\n",
    "\n",
    "                dfmesh = pd.DataFrame(MESHsplit).rename(columns={0: 'pmids',1:'mesh',2:'qualifier',3:'major'}).drop_duplicates()\n",
    "\n",
    "                #statistics\n",
    "                pubmed_pmidmesh = len(dfmesh[['pmids','mesh']].drop_duplicates())\n",
    "                pubmed_mesh_qualifier_major = len(MESH.mesh.drop_duplicates())\n",
    "                pubmed_mesh = len(dfmesh.mesh.drop_duplicates())\n",
    "\n",
    "                #print('\\npmid-with-mesh:',len(MESH.pmids.drop_duplicates()))\n",
    "                #^print('\\n[pmid-mesh]_count:',pubmed_pmidmesh)\n",
    "                #^print('[mesh-qualifier-major]_count:',pubmed_mesh_qualifier_major)\n",
    "                #^print('[mesh]_count:',pubmed_mesh)\n",
    "\n",
    "                #simple dataframe [pmid-mesh]\n",
    "                #^print('\\nreminder:')\n",
    "                #^print(gene,'rsid litvar1 (pmid>1):', lit1_rsid_f)\n",
    "                #^print('pmid litvar1 (pmid>1):', lit1_pmid_f)\n",
    "\n",
    "                pmidmesh = dfmesh[['pmids','mesh']].drop_duplicates()\n",
    "                pmidmesh['pmids'] = pmidmesh['pmids'].astype(str) #convert pmid type in str\n",
    "                #^display(dfmesh.head(20))\n",
    "                #dfmesh.to_csv(''+gene+' pmid-mesh.csv')\n",
    "\n",
    "                #Analyze enrichment with groupby.describe method------------------------------------------\n",
    "                #Add rsid coulmn con merge\n",
    "                rspmidmesh_merge = pd.merge(pmidmesh, rsidpmid, on= 'pmids', how='inner').drop_duplicates().reindex(columns=['pmids', 'rsid', 'mesh'])\n",
    "                #rspmidmesh_merge['pmids'] = rspmidmesh_merge['pmids'].astype(str)\n",
    "\n",
    "                ### groupby.describe analysis by mesh\n",
    "                meshrspmidmerge_count = rspmidmesh_merge.groupby('mesh').describe().reset_index()\n",
    "                meshrspmidmerge_count.columns = meshrspmidmerge_count.columns.to_flat_index()\n",
    "                #to handle generate df.groupby.describe, convert Multicolumn to single column\n",
    "                #https://datascientyst.com/flatten-multiindex-in-pandas/\n",
    "                new_column_names = ['mesh', 'pmid-count', 'pmid-unique','pmid-top','pmid-freq','rsid-count', 'rsid-unique','rsid-top','rsid-freq']\n",
    "                meshrspmidmerge_count.columns = new_column_names\n",
    "\n",
    "                meshrspmidmerge_count_short = meshrspmidmerge_count[['mesh','pmid-unique','rsid-unique']]\n",
    "                #pmidmeshintmerge2meshlesssort = pmidmeshintmerge2meshless.sort_values(by='pmid-unique',ascending=False).reset_index(drop=True)\n",
    "\n",
    "                # add frequency\n",
    "                totalpmid_count = len(pmidmesh.pmids.drop_duplicates())\n",
    "                meshrspmidmerge_count_short_freq = meshrspmidmerge_count_short.copy()\n",
    "                meshb_frq = meshrspmidmerge_count_short_freq.loc[:,'pmid-unique'].astype(float)/totalpmid_count\n",
    "                meshrspmidmerge_count_short_freq.loc[:,'mesh frequency'] = round(meshb_frq,3)#*100\n",
    "                meshrspmidmerge_count_short_freq_sort = meshrspmidmerge_count_short_freq.sort_values(by='pmid-unique',ascending=False).reset_index(drop=True)\n",
    "\n",
    "                top10mesh_all = meshrspmidmerge_count_short_freq_sort['mesh'][:10].tolist()\n",
    "                #display(meshrspmidmerge_count_short_freq_sort.head(20))\n",
    "\n",
    "                ### groupby.describe analysis by rsid------------------\n",
    "                rspmidmeshmerge_count = rspmidmesh_merge.groupby('rsid').describe().reset_index()\n",
    "                rspmidmeshmerge_count.columns = rspmidmeshmerge_count.columns.to_flat_index()\n",
    "                new_column_names = ['rsid', 'pmid-count', 'pmid-unique','pmid-top','pmid-freq','mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "                rspmidmeshmerge_count.columns = new_column_names\n",
    "\n",
    "                rsid_pmid10 = len(rspmidmeshmerge_count[rspmidmeshmerge_count['pmid-unique']>10])\n",
    "                rsid_pmid50 = len(rspmidmeshmerge_count[rspmidmeshmerge_count['pmid-unique']>50])\n",
    "                rsid_pmid100 = len(rspmidmeshmerge_count[rspmidmeshmerge_count['pmid-unique']>100])\n",
    "\n",
    "                rspmidmeshmerge_count_short = rspmidmeshmerge_count[['rsid','pmid-unique','mesh-unique']]\n",
    "                rspmidmeshmerge_count_short_sort = rspmidmeshmerge_count_short.sort_values(by='pmid-unique', ascending= False).reset_index(drop=True)\n",
    "                top10rsid_all = rspmidmeshmerge_count_short_sort['rsid'].iloc[:10].tolist()\n",
    "\n",
    "                if save_plot == True:\n",
    "                    # create a scatter plot-----------------------------------------\n",
    "                    x1 = meshrspmidmerge_count_short_freq_sort['mesh'].head(30)\n",
    "                    y1 = meshrspmidmerge_count_short_freq_sort['pmid-unique'].head(30)\n",
    "                    plt.figure(figsize=(5, 8))\n",
    "                    plt.title('Scatter Plot: '+gene+' pmid-mesh (total)', loc='center',pad=10)\n",
    "                    plt.scatter(y1, x1)\n",
    "                    plt.gca().invert_yaxis()\n",
    "                    #plt.yticks(rotation=90)\n",
    "                    plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "                    plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "                    ax = plt.gca()\n",
    "                    ax.xaxis.set_label_position('top')\n",
    "                    plt.savefig(db_name+'/'+gene+'_mesh_plot_'+timestamp+'_total.png',dpi=120, bbox_inches = \"tight\")\n",
    "                    plt.close()\n",
    "\n",
    "                if save_studytype_data == True:\n",
    "                    # GET STUDY TYPE from NBIB----------------------------------------\n",
    "                    dfbib = pd.DataFrame(ref)\n",
    "                    dfbib.pubmed_id = dfbib.pubmed_id.astype('str')\n",
    "                    if 'publication_types' in dfbib.columns and len(dfbib['publication_types'])>1:\n",
    "                        dfbib_studyty = dfbib[['pubmed_id','publication_types']].dropna().reset_index(drop=True)\n",
    "\n",
    "                        #PMID-Studytype table build:\n",
    "                        df_studytype = []\n",
    "                        for i in range(len(dfbib_studyty)):\n",
    "                            for studytype in dfbib_studyty['publication_types'][i]:\n",
    "                                out = dfbib_studyty['pubmed_id'][i], studytype\n",
    "                                df_studytype.append(out)\n",
    "                        STUDYT = pd.DataFrame(df_studytype).rename(columns={0: 'pmids',1:'study_type'})\n",
    "                        mask_st = STUDYT['study_type'].str.contains('Research Support|Journal Article')\n",
    "                        STUDYTless = STUDYT[~mask_st].reset_index(drop=True)\n",
    "\n",
    "                        mask_lessing = STUDYT['pmids'].isin(STUDYTless['pmids'])\n",
    "                        STUDYTdiff = STUDYT[~mask_lessing].reset_index(drop=True)\n",
    "                        STUDYTdiff['study_type2'] = 'Unknown'\n",
    "                        STUDYTdiff = STUDYTdiff[['pmids','study_type2']].rename(columns={'study_type2':'study_type'}).drop_duplicates().reset_index(drop=True)\n",
    "                        #len(STUDYTless.pmids.drop_duplicates()), len(dfbib.pubmed_id.drop_duplicates())\n",
    "                        STUDYTconcat = pd.concat([STUDYTless, STUDYTdiff], ignore_index=True)\n",
    "                        STUDYTconcat#.pmids.drop_duplicates()\n",
    "\n",
    "                        #study type count:\n",
    "                        STUDYTless_count = STUDYTconcat.groupby('study_type').describe().reset_index()\n",
    "                        STUDYTless_count.columns = STUDYTless_count.columns.to_flat_index()\n",
    "                        new_column_names = ['study_type', 'pmid-count', 'pmid-unique','pmid-top','pmid-freq']\n",
    "                        STUDYTless_count.columns = new_column_names\n",
    "                        STUDYTless_count_sort = STUDYTless_count.sort_values(by= 'pmid-count',ascending=False)\n",
    "\n",
    "                        # save data\n",
    "                        STUDYTconcat.to_csv(db_name+'/'+gene+'_lit1pmid_studytype.csv')\n",
    "                    else:\n",
    "                        print(gene+' no publication_types in nbib')\n",
    "                        pass\n",
    "\n",
    "#SAVE TABLES-----------------------------------------------------------\n",
    "                timestamp = time2.strftime('%Y%m%d%H%M%S')\n",
    "                # save accessory data:\n",
    "                if save_accessory_data == True:\n",
    "                    dfsort[[\"_id\",\"rsid\",\"pmids_count\"]].to_csv(db_name+'/'+gene+'_litvar2_variants4gene.csv')\n",
    "                    rsidpmid.to_csv(db_name+'/'+gene+'_litvar1_rsids2pmids.csv') #lit1 [rsid-pmid]\n",
    "                    #rsidpmidcountfsort #lit1 pmid count\n",
    "\n",
    "                    meshrspmidmerge_count_short_freq_sort.to_csv(db_name+'/'+gene+'_mesh_pmidrsid_count.csv')\n",
    "\n",
    "                #complete_df with concat:\n",
    "                #import gene-rsidpmidmesh and gene-rsidpmid\n",
    "                dfmesh['pmids'] = dfmesh['pmids'].astype(str)\n",
    "                rsidpmid['pmids'] = rsidpmid['pmids'].astype(str)\n",
    "\n",
    "                # add a rsid-merger to dfmesh\n",
    "                gene_rsidpmidmesh = pd.merge(rsidpmid, dfmesh, on='pmids')\n",
    "                gene_rsidpmidmesh['gene'] = gene\n",
    "\n",
    "                gene_df = pd.DataFrame(gene_rsidpmidmesh)\n",
    "                complete_df = pd.concat([complete_df, gene_rsidpmidmesh])\n",
    "\n",
    "                #complete_nbibtable with concat:\n",
    "                dfbib['gene'] = gene\n",
    "                complete_nbibtable = pd.concat([complete_nbibtable, dfbib])\n",
    "                #pyperclip.copy(str(dfbib.columns.to_list()))\n",
    "\n",
    "                #old storage method\n",
    "                    #dfmesh.to_csv(db_name+'/'+gene+'_pubmed_pmidmesh.csv')\n",
    "                    #dfbib.to_csv(db_name+'/'+gene+'_nbib_table.csv')\n",
    "\n",
    "#REPORT-----------------------------------------------------------------\n",
    "                time_omega = datetime.now()\n",
    "                full_runtime = time_omega - time_alpha\n",
    "                #^print('total runtime:', full_runtime)\n",
    "                print(gene + '_runtime:', full_runtime)\n",
    "                nbib_seconds = runtime.total_seconds()\n",
    "                total_seconds = full_runtime.total_seconds()\n",
    "                full_runtime_str = str(full_runtime).split('.')[0]\n",
    "\n",
    "                report = {'ncbi_dbsnp': NCBI_dbSNP,\n",
    "                          'lit2_variant': lit2_variant,\n",
    "                          'lit2_variant_norsid': lit2_variant_norsid,\n",
    "                          'lit2_rsid': lit2_rsid,\n",
    "                          'lit2_rsid_plus1': lit2_rsid_f,\n",
    "                          'lit1_rsid': lit1_rsid,\n",
    "                          #'lit1_raw_pmid': lit1_raw_pmid,\n",
    "                          #'lit1_rsid_pmid': lit1_rsid_pmid,\n",
    "                          'lit1_rsid_pmid_plus1': lit1_rsid_f,\n",
    "                          #lit1_rsid_pmid=1': lit1_rsid_m,\n",
    "                          'lit1_pmid': lit1_pmid,\n",
    "                          'lit1_pmid_pmid_plus1': lit1_pmid_f,\n",
    "                          'pubmed_pmid_query': pubmed_pmid_query,\n",
    "                          'nbib_objects': nbib_objects,\n",
    "                          'nbib_objects_withdescriptors': nbib_objects_withdescriptors,\n",
    "                          'pubmed_pmid': pubmed_pmid_nbib,\n",
    "                          'pubmed_pmid_withmesh': pubmed_pmid_nbib_yesmesh,\n",
    "                          #'pubmed_pmid_nomesh':pubmed_pmid_nbib_nomesh,\n",
    "                          'pubmed_pmidmesh': pubmed_pmidmesh,\n",
    "                          'pubmed_mesh_qualifier_major': pubmed_mesh_qualifier_major,\n",
    "                          'pubmed_mesh': pubmed_mesh,\n",
    "                          'rsid_pmid10': rsid_pmid10,\n",
    "                          'rsid_pmid50': rsid_pmid50,\n",
    "                          'rsid_pmid100': rsid_pmid100,\n",
    "                          'top10mesh_all': str(top10mesh_all),\n",
    "                          'top10rsid_all': str(top10rsid_all),\n",
    "                          'pubmed_runtime': duration,\n",
    "                          'total_runtime': full_runtime_str,\n",
    "                          'time_stamp': time2\n",
    "                          }\n",
    "\n",
    "                df_report = pd.DataFrame(report, index=[gene]).transpose()\n",
    "\n",
    "                # generate fist report.csv\n",
    "                if os.path.isfile(db_name+'/GRPM_report.csv'):\n",
    "                    dfL = pd.read_csv(db_name+'/GRPM_report.csv', index_col=0)\n",
    "                    dfL = pd.concat([dfL, df_report], axis=1)\n",
    "                    dfL.to_csv(db_name+'/GRPM_report.csv')\n",
    "                else:\n",
    "                    df_report.to_csv(db_name+'/GRPM_report.csv')  # solo la prima volta\n",
    "\n",
    "                #Update gene values\n",
    "                GRPM_report = pd.read_csv(db_name+'/GRPM_report.csv', index_col=0)\n",
    "                if gene + '.1' in GRPM_report.columns:\n",
    "                    GRPM_report = GRPM_report.drop(columns=gene)\n",
    "                    GRPM_report = GRPM_report.rename(columns={gene + '.1': gene})\n",
    "                    GRPM_report.to_csv(db_name+'/GRPM_report.csv')\n",
    "                    print(gene,'already in db')\n",
    "\n",
    "            else:\n",
    "                print(gene + ' no descriptors in nbib')\n",
    "                time.sleep(0.8)\n",
    "                pass\n",
    "        else:\n",
    "            print(gene + ' no results on litvar1')\n",
    "            time.sleep(0.8)\n",
    "            pass\n",
    "    else:\n",
    "        print(gene + ' no results on litvar2')\n",
    "        pass\n",
    "\n",
    "    if request_counter > 9000:\n",
    "        dada = 2\n",
    "        #print('Allert! Reaching pubmed request limit')\n",
    "    if request_counter > 9950:\n",
    "        #print('Request limit reached. Wait \\'till tomorrow!')\n",
    "        time_finish = datetime.now()\n",
    "        time_batch = time_finish - time_start\n",
    "        time_batch_str = str(time_batch).split('.')[0]\n",
    "        #print('time batch:', time_batch_str)\n",
    "        #break\n",
    "\n",
    "complete_df = complete_df.reindex(columns=['gene','rsid', 'pmids', 'mesh', 'qualifier', 'major'])\n",
    "complete_df.to_csv(db_name+'/grpm_table_output.csv')\n",
    "\n",
    "complete_nbibtable = complete_nbibtable.reindex(columns=['gene','pubmed_id', 'citation_owner', 'nlm_status', 'last_revision_date', 'electronic_issn', 'linking_issn', 'journal_volume', 'journal_issue', 'publication_date', 'title', 'abstract', 'authors', 'language', 'grants', 'publication_types', 'electronic_publication_date', 'place_of_publication', 'journal_abbreviated', 'journal', 'nlm_journal_id', 'descriptors', 'pmcid', 'keywords', 'conflict_of_interest', 'received_time', 'revised_time', 'accepted_time', 'pubmed_time', 'medline_time', 'entrez_time', 'pii', 'doi', 'publication_status', 'print_issn', 'pages'])\n",
    "complete_nbibtable.to_csv(db_name+'/complete_nbibtable.csv')\n",
    "\n",
    "time_finish = datetime.now()\n",
    "time_batch = time_finish - time_start\n",
    "time_batch_str = str(time_batch).split('.')[0]\n",
    "print('gene batch:', len(genes))\n",
    "print('time batch:', time_batch_str)\n",
    "print('runtime/gene:', time_batch/len(genes))\n",
    "print('request_counter:', request_counter,' (limit: 10.000/day)')\n",
    "gene_counter += len(genes)\n",
    "print('requests/gene:', request_counter/gene_counter)\n",
    "print(time_finish)\n",
    "\n",
    "###LIMITS PubMed Programming Utilities (PMU)\n",
    "#10 requests/second\n",
    "#10,000 requests/day\n",
    "\n",
    "#ChatGPT:As an AI language model, I should advise you that web scraping of PubMed is not allowed as it violates PubMed's terms of service.\n",
    "# PubMed provides an API called PubMed Programming Utilities (PMU) that allows developers to access PubMed data programmatically. Using the API is the preferred way to obtain data from PubMed.\n",
    "# Web scraping of PubMed may result in your IP address being temporarily or permanently blocked from accessing PubMed data. It is important to respect PubMed's terms of service and guidelines when accessing their data to avoid any legal repercussions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('grpm report:')\n",
    "display(pd.read_csv(db_name+'/GRPM_report.csv', index_col=0).T)\n",
    "print('grpm table genes:', len(pd.read_csv(db_name+'/grpm_table_output.csv').gene.drop_duplicates()))\n",
    "display(pd.read_csv(db_name+'/grpm_table_output.csv'))\n",
    "#print('nbib table:')\n",
    "#display(pd.read_csv(db_name+'/complete_nbibtable.csv',index_col=0))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.Series(genes).to_clipboard(index= False)\n",
    "pyperclip.copy(str(genes.to_list()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OG\n",
    "on genes1_part1 (1187)\n",
    "    no results on\n",
    "        litvar: 214\n",
    "        litvar2: 43\n",
    "        litvar1: 126\n",
    "        nbib: 11\n",
    "\n",
    "on genes1_part2 (---)\n",
    "    no results on\n",
    "        litvar: 230\n",
    "        litvar2: 27\n",
    "        litvar1: 201\n",
    "        nbib: 13"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize GRPM_report.csv\n",
    "GRPM_report = pd.read_csv(db_name+'/GRPM_report.csv', index_col=0).transpose().reset_index().rename(columns={'index':'gene'})\n",
    "\n",
    "GRPM_report[['ncbi_dbsnp', 'lit2_variant', 'lit2_variant_norsid','lit2_rsid','lit2_rsid_plus1', 'lit1_rsid', 'lit1_rsid_pmid_plus1','lit1_pmid', 'lit1_pmid_pmid_plus1','pubmed_pmid_query',    'nbib_objects', 'nbib_objects_withdescriptors', 'pubmed_pmid',              'pubmed_pmid_withmesh', 'pubmed_pmidmesh','pubmed_mesh_qualifier_major','pubmed_mesh', 'rsid_pmid10','rsid_pmid50', 'rsid_pmid100' ]] = GRPM_report[['ncbi_dbsnp','lit2_variant', 'lit2_variant_norsid','lit2_rsid', 'lit2_rsid_plus1','lit1_rsid', 'lit1_rsid_pmid_plus1', 'lit1_pmid','lit1_pmid_pmid_plus1', 'pubmed_pmid_query','nbib_objects', 'nbib_objects_withdescriptors', 'pubmed_pmid', 'pubmed_pmid_withmesh', 'pubmed_pmidmesh', 'pubmed_mesh_qualifier_major', 'pubmed_mesh', 'rsid_pmid10', 'rsid_pmid50', 'rsid_pmid100' ]].astype(int)\n",
    "\n",
    "#display(GRPM_report_less.sort_values(by= 'matching_pmids',ascending=False))\n",
    "GRPM_report.sort_values(by='lit1_pmid',ascending = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "# create histogram\n",
    "GRPM_report_sort = GRPM_report.sort_values(by= 'pubmed_pmid',ascending=False)\n",
    "#GRPM_report_sort = GRPM_report_01.sort_values(by= 'pubmed_pmid',ascending=False)\n",
    "\n",
    "x = GRPM_report_sort.gene.iloc[:40]\n",
    "y = GRPM_report_sort['pubmed_pmid'].iloc[:40]\n",
    "plt.figure(figsize=(4, 8))\n",
    "plt.title('PMIDs in Database', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "#plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "plt.ylabel('genes')\n",
    "plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "#plt.savefig(r'C:\\Users\\Public\\Database_SSD\\GRPX_heavyload (RunOnly)\\logs\\Database_Gene('+str(len(GRPM_filtered_report.gene))+')-PMID_filtered.png',dpi=300, bbox_inches = \"tight\")\n",
    "#plt.clf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#NBIB PROBLEM SOLVER---------------------------------\n",
    "# #replace malformed lines\n",
    "fullnbib= fullnbib.replace('2007/09/31','2007/09/30')\n",
    "ref = nbib.read(fullnbib)\n",
    "dfbib = pd.DataFrame(ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#NBIB PROBLEM SOLVER [History] ---------------------------------\n",
    "with open('nbib report '+gene+'.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(fullnbib)\n",
    "with open('nbib report '+gene+'_FIXED.txt', 'r', encoding='utf-8') as file:\n",
    "    fullnbib = file.read() # --> not work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize GENES in DATABASE\n",
    "# Get gene to Analyze\n",
    "gene_list = pd.Series(GRPM_report.gene).to_list()\n",
    "#gene_list.to_clipboard(index=False)\n",
    "import pyperclip\n",
    "pyperclip.copy(str(gene_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "['MT-ND1', 'MT-ND2', 'MT-CO1', 'MT-CO2', 'MT-ATP8']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#CODE DUMP-------------------------------------------------\n",
    "### EUTILS GET STUDY TYPE MODULE\n",
    "#https://biopython.org/docs/1.76/api/Bio.Entrez.html\n",
    "def get_study_type(pmids):\n",
    "    Entrez.email = 'your_email@your_domain.com'\n",
    "    handle = Entrez.esummary(db='pubmed', id=','.join(pmids), retmode='xml')\n",
    "    records = Entrez.parse(handle)\n",
    "    study_types = []\n",
    "    for record in records:\n",
    "        article_types = record['PubTypeList']\n",
    "        if 'Randomized Controlled Trial' in article_types:\n",
    "            study_types.append('Randomized Controlled Trial')\n",
    "        elif 'Controlled Clinical Trial' in article_types:\n",
    "            study_types.append('Controlled Clinical Trial')\n",
    "        elif 'Cohort Studies' in article_types:\n",
    "            study_types.append('Cohort Study')\n",
    "        elif 'Case-Control Studies' in article_types:\n",
    "            study_types.append('Case-Control Study')\n",
    "        elif 'Review' in article_types:\n",
    "            study_types.append('Review')\n",
    "        elif 'Clinical Trial' in article_types:\n",
    "            study_types.append('Clinical Trial')\n",
    "        elif 'Meta-Analysis' in article_types:\n",
    "            study_types.append('Meta-Analysis')\n",
    "        elif 'Multicenter Study' in article_types:\n",
    "            study_types.append('Multicenter Study')\n",
    "        else:\n",
    "            study_types.append('Unknown')\n",
    "    return study_types\n",
    "\n",
    "pmidlist = list(pmidmesh['pmids'].drop_duplicates())\n",
    "genepmids_str = list(map(str, pmidlist))\n",
    "study_type = get_study_type(genepmids_str)\n",
    "pmids_studytype = pd.DataFrame(list(zip(genepmids_str, study_type)), columns=[gene + '_PMID', 'study type'])\n",
    "request_counter += 1\n",
    "\n",
    "#study type count:\n",
    "pmids_studytype_count = pmids_studytype.groupby('study type').describe().reset_index()\n",
    "pmids_studytype_count.columns = pmids_studytype_count.columns.to_flat_index()\n",
    "new_column_names = ['study_type', 'pmid-count', 'pmid-unique', 'pmid-top', 'pmid-freq']\n",
    "pmids_studytype_count.columns = new_column_names\n",
    "pmids_studytype_countsort = pmids_studytype_count.sort_values(by='pmid-count', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
