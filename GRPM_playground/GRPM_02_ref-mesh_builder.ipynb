{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ref MeSH List Builder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " This notebook allows you to create Mesh lists for the GRPMX system starting with a topic of choice.\n",
    " The system is based on the complete MESH datasheet.\n",
    " The system uses ChatGTP to provide the initial \"meshes.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "#Import Modules\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "import IPython\n",
    "#import nbib\n",
    "#import requests as rq\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#from Bio import Entrez\n",
    "#Entrez.email = \"your_email@example.com\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Mesh dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load MESH dataframe:\n",
    "if os.path.exists('ref-mesh-archive/MESH.csv'):\n",
    "    df = pd.read_csv('ref-mesh-archive/MESH.csv')\n",
    "else:\n",
    "    print(\"no MESH.csv avalable jump to 'Import/create MESH-STY-LITVAR' section\")\n",
    "\n",
    "#info on Proprieties: https://bioportal.bioontology.org/ontologies/MESH?p=properties\n",
    "#Mesh Browser: https://meshb.nlm.nih.gov/search?searchMethod=SubString&searchInField=termDescriptor&sort=&size=20&searchType=exactMatch&from=0&q=Diabetes\n",
    "\n",
    "# AQL: Allowable Qualifiers\n",
    "AQL = [\"blood (BL)\",\"cerebrospinal fluid (CF)\",\"chemically induced (CI)\",\"classification (CL)\",\"complications (CO)\",\"congenital (CN)\",\"diagnosis (DI)\",\"diagnostic imaging (DG)\",\"diet therapy (DH)\",\"drug therapy (DT)\",\"economics (EC)\",\"embryology (EM)\",\"enzymology (EN)\",\"epidemiology (EP)\",\"ethnology (EH)\",\"etiology (ET)\",\"genetics (GE)\",\"history (HI)\",\"immunology (IM)\",\"metabolism (ME)\",\"microbiology (MI)\",\"mortality (MO)\",\"nursing (NU)\",\"parasitology (PS)\",\"pathology (PA)\",\"physiopathology (PP)\",\"prevention & control (PC)\",\"psychology (PX)\",\"radiotherapy (RT)\",\"rehabilitation (RH)\",\"surgery (SU)\",\"therapy (TH)\",\"urine (UR)\",\"veterinary (VE)\",\"virology (VI)\"]\n",
    "aql=pd.DataFrame(AQL)\n",
    "aql"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df.iloc[:3]\n",
    "df.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#How many Mesh has mapping qualifier (subheadings)\n",
    "all = df['Preferred Label'].drop_duplicates()\n",
    "mapping_qf = df[['Mapping qualifier of','Preferred Label']].drop_duplicates().dropna()\n",
    "has = df[['Has mapping qualifier','Preferred Label']].drop_duplicates().dropna()\n",
    "has_len = has['Preferred Label'].nunique()\n",
    "print('Total number of mesh terms:\\n', df['Preferred Label'].nunique())\n",
    "print('How many Mesh has mapping qualifier (subheadings):\\n',has_len)\n",
    "print(' %', round((has_len/len(all)*100),3))\n",
    "#for i in df.columns:\n",
    "#    print(df[i].drop_duplicates())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# search for single exact value\n",
    "mesh = 'Disease or Syndrome'\n",
    "print('search for single exact value:')\n",
    "df[df['Preferred Label']==mesh]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Extract subset of Mesh\n",
    "str_1 = 'Fabry'\n",
    "str_2 = 'Lysosomal Sto'\n",
    "str_3 = ''\n",
    "str_full = str_2 or str_1 or str_3\n",
    "\n",
    "ref_search = df[df['Preferred Label'].str.contains(str_full).fillna(False)]\n",
    "#sem.to_csv('ref-mesh-archive/ClassID-STY_SemanticTypes.csv')\n",
    "print('look for mesh containing \"',str_1,',',str_2,',',str_3,  '\":')\n",
    "ref_search\n",
    "#ref_search.to_csv('ref-mesh-archive/ref_mesh_LSD.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantic Types Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Extract subset of all semantic types\n",
    "df_sem = df[df['Class ID'].str.contains('STY').fillna(False)]\n",
    "df_sem\n",
    "#sem.to_csv('ref-mesh-archive/ClassID-STY_SemanticTypes.csv')\n",
    "#df['Class ID'].to_csv('ref-mesh-archive/ClassID.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Characterize a Semantic Type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#how many mesh for semantic type?\n",
    "semantictype = 'T192'\n",
    "sam_label = df[df['Class ID'].str.contains(semantictype).fillna(False)].reset_index().at[0,'Preferred Label']\n",
    "# Give me Semantic Type name for STY ID\n",
    "print('semantic type:', sam_label)\n",
    "\n",
    "mask2 = df['Semantic Types'].str.contains(semantictype).fillna(False)\n",
    "mesh_sem = df[mask2][['Class ID','Preferred Label','Definitions','Semantic Types']]\n",
    "print('number of mesh:', len(mesh_sem))\n",
    "print('\\nMesh for \"',semantictype,sam_label,'\" semantic type:')\n",
    "\n",
    "mesh_sem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Characterize single mesh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# search for single Mesh\n",
    "mesh_classid ='D004048'\n",
    "maskmesh = df['Class ID'].str.contains(mesh_classid).fillna(False)\n",
    "\n",
    "sty_classid = df[maskmesh]['Semantic Types'].reset_index().iat[0, 1][-4:]\n",
    "#print('note: this method catches only meshes that has sam_label at the in the felad \"sematic Type\" for multy sem type meshes')\n",
    "\n",
    "masksty = df['Class ID'].str.contains(sty_classid).fillna(False)\n",
    "\n",
    "masked = df[maskmesh][['Semantic Types','Preferred Label','Definitions']].reset_index()\n",
    "\n",
    "#get mesh definition:\n",
    "print('mesh:', masked.at[0,'Preferred Label'],\n",
    "      '\\nsemantic type:',df[masksty].reset_index().at[0,'Preferred Label'],sty_classid,\n",
    "      '\\ndescrition:',masked.at[0,'Definitions'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Row aggregation\n",
    "data_mesh = []\n",
    "data_sty = ['T021','T116']\n",
    "dfd = pd.DataFrame()\n",
    "for i in data_sty:\n",
    "    #mask = df['Class ID'].str.contains(i).fillna(False) # aggregate sty\n",
    "    mask = df['Semantic Types'].str.contains(i).fillna(False) # aggregate mesh for sty\n",
    "    #dfd = dfd.append(df[mask])\n",
    "    dfd = pd.concat([dfd, df[mask]])\n",
    "dfd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import/create MESH-STY and MESH-STY-LITVAR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set pram\n",
    "import_mesh_sty = True\n",
    "\n",
    "# concat the exploded mesh table or import prearranged csv\n",
    "if os.path.isfile('ref-mesh-archive/MESH_STY.csv') and import_mesh_sty == True:\n",
    "    mesh_large_df_sty = pd.read_csv('ref-mesh-archive/MESH_STY.csv', index_col=0)\n",
    "    print('MESH_STY sty imported from csv')\n",
    "else:\n",
    "    if import_mesh_sty == True:\n",
    "        if os.path.exists('ref-mesh-archive/MESH.csv'):\n",
    "            # correct list format\n",
    "            df['STY_ID'] = df['Semantic Types'].str.replace(r'http://purl.bioontology.org/ontology/STY/','', regex = False)\n",
    "            start_word = '[\\\"'\n",
    "            end_word = '\\\"]'\n",
    "            df['STY_ID'] = f'{start_word} ' + df['STY_ID'] + f' {end_word}'\n",
    "            df['STY_ID'] = df['STY_ID'].str.replace(' ','', regex = False)\n",
    "            df['STY_ID'] = df['STY_ID'].str.replace('|','\\\",\\\"', regex = False)\n",
    "            print(df['STY_ID'].isna().sum())\n",
    "            df.dropna(subset=['STY_ID'], inplace=True)\n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "            type(df.STY_ID[0])\n",
    "\n",
    "            from ast import literal_eval\n",
    "            df['STY_ID'] = df['STY_ID'].apply(literal_eval)\n",
    "\n",
    "            # Import exhcange table sty-code\n",
    "            sty = pd.read_csv('ref-mesh-archive/MeshSTY-code.csv',sep=';')\n",
    "            sty = sty.rename(columns={'ID':'Semantic Types'})\n",
    "            sty = sty.rename(columns={'ID':'Semantic Types'})\n",
    "            print(sty['Semantic Types'].nunique())\n",
    "\n",
    "            #mesh_large = pd.DataFrame()\n",
    "            mesh_large = []\n",
    "            for i in range(len(df)):\n",
    "                for sem in df['STY_ID'][i]: #dfrspost = mother table\n",
    "                    out = df['Preferred Label'][i],df['Class ID'][i],sem,df['Synonyms'][i],df['Parents'][i],df['CUI'][i],df['AQL'][i],df['TERMUI'][i]\n",
    "                    mesh_large.append(out)\n",
    "                    #df_out = pd.DataFrame(out)\n",
    "                    #pd.concat([mesh_large, df_out])\n",
    "\n",
    "            mesh_large_df = pd.DataFrame(mesh_large)\n",
    "            new_col_names = ['Preferred Label','Class ID','Semantic Types','Synonyms','Parents','CUI','AQL','TERMUI']\n",
    "            mesh_large_df.columns = new_col_names\n",
    "            mesh_large_df\n",
    "            ## Add STY Labels\n",
    "            mesh_large_df_sty = pd.merge(mesh_large_df, sty, on='Semantic Types', how='inner').reset_index(drop=True)\n",
    "            #Add rsid coulmn con merge\n",
    "            mesh_large_df_sty = mesh_large_df_sty.rename(columns={'Preferred Label_y':'Semantic Types Label','Preferred Label_x':'Preferred Label'})\n",
    "\n",
    "            mesh_large_df_sty = mesh_large_df_sty[['Preferred Label', 'Semantic Types Label', 'Class ID', 'Semantic Types', 'Synonyms', 'Parents', 'CUI', 'AQL', 'TERMUI']]\n",
    "            mesh_large_df_sty.to_csv('ref-mesh-archive/MESH_STY.csv')\n",
    "            print('MESH_STY created')\n",
    "        else:\n",
    "            print('MESH.csv not avalable')\n",
    "\n",
    "\n",
    "#### Create MESH-STY-LITVAR subset from MESH-STY.csv\n",
    "if os.path.exists('ref-mesh-archive/MESH_STY_LITVAR1.csv'):\n",
    "    mesh_litvar_sty = pd.read_csv('ref-mesh-archive/MESH_STY_LITVAR1.csv',index_col=0)\n",
    "    print('MESH_STY_LITVAR1 imported from csv')\n",
    "else:\n",
    "    grpm_mesh = pd.read_csv('ref-mesh-archive/grpm_db_mesh.csv', index_col=0)\n",
    "    mask = mesh_large_df_sty['Preferred Label'].isin(grpm_mesh.mesh)\n",
    "    #mesh_large_df_sty['Preferred Label'].nunique()\n",
    "    mesh_litvar_sty = mesh_large_df_sty[mask]\n",
    "    mesh_litvar_sty.to_csv('ref-mesh-archive/MESH_STY_LITVAR1.csv')\n",
    "    print('MESH_STY_LITVAR1 created')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'mesh_large_df_sty' in globals():\n",
    "    print('MESH_STY', len(mesh_large_df_sty['Preferred Label']), ' rows')\n",
    "    print('MESH_STY', mesh_large_df_sty['Preferred Label'].nunique(), 'mesh')\n",
    "\n",
    "print('MESH_STY_LITVAR1', len(mesh_litvar_sty['Preferred Label']), ' rows')\n",
    "print('MESH_STY_LITVAR1', mesh_litvar_sty['Preferred Label'].nunique(), 'mesh')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## ADD mesh_id col\n",
    "mesh_litvar_sty['mesh_id'] = mesh_litvar_sty['Class ID'].str.replace('http://purl.bioontology.org/ontology/MESH/', '')\n",
    "mesh_litvar_sty['mesh_id']\n",
    "mesh_litvar_sty.columns\n",
    "new_order = ['Preferred Label', 'Semantic Types Label', 'Class ID', 'mesh_id', 'Semantic Types',\n",
    "             'Synonyms', 'Parents', 'CUI', 'AQL', 'TERMUI']\n",
    "mesh_litvar_sty = mesh_litvar_sty[new_order]\n",
    "mesh_litvar_sty.to_csv('ref-mesh-archive/MESH_STY_LITVAR1.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## add mesh_id to grpm db"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grpm_b_df = pd.read_csv('grpm_db_pcg/grpm_table_output.csv',index_col=0)\n",
    "grpm_b_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mesh_id_ref_df = mesh_litvar_sty[['Preferred Label','mesh_id']]\n",
    "mesh_id_ref_df\n",
    "grpm_db_merge_id = pd.merge(grpm_b_df, mesh_id_ref_df, left_on='mesh', right_on='Preferred Label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#grpm_db_merge_id[['mesh','Preferred Label','mesh_id']].drop_duplicates()\n",
    "grpm_db_merge_id = grpm_db_merge_id.drop('Preferred Label', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grpm_db_merge_id.columns\n",
    "new_order = ['gene', 'rsid', 'pmids', 'mesh_id']\n",
    "grpm_db_merge_id = grpm_db_merge_id[new_order].drop_duplicates()\n",
    "grpm_db_merge_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grpm_db_merge_id.to_csv('grpm_db_pcg/grpm_table_output_id.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check = pd.read_csv('grpm_db_pcg/grpm_table_output_id.csv', index_col=0)\n",
    "check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sorting Mesh-STY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'mesh_large_df_sty' in globals():\n",
    "    #modulo groupby and bar\n",
    "    mesh_large_df_sty_less = mesh_large_df_sty[['Preferred Label','Semantic Types Label']]\n",
    "\n",
    "    ### groupby.describe analysis by rsid--------------------\n",
    "    mesh_large_df_sty_less_count = mesh_large_df_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    mesh_large_df_sty_less_count.columns = mesh_large_df_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    mesh_large_df_sty_less_count.columns = new_column_names\n",
    "\n",
    "    mesh_large_df_sty_less_count_sort = mesh_large_df_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "    print('MESH_STY')\n",
    "    print(len(mesh_large_df_sty_less_count_sort))\n",
    "\n",
    "mesh_large_df_sty_less_count_sort[['mesh-count','Semantic Types Label']]#.to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Graph Barh\n",
    "num = 20\n",
    "x = mesh_large_df_sty_less_count_sort['Semantic Types Label'].iloc[:num]\n",
    "y = mesh_large_df_sty_less_count_sort['mesh-count'].iloc[:num]\n",
    "plt.figure(figsize=(4, len(x) *0.25))\n",
    "plt.title('Global Mesh- Semantic Type enrichment', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "#plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "plt.ylabel('Semantic Types')\n",
    "plt.xlabel('mesh', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "# use log scale\n",
    "plt.gca().set_xscale('log')\n",
    "#plt.savefig('Reference Mesh- Semantic Type enrichment.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#'Global \"Mesh-Semantic Type\" Zipf s law'\n",
    "x = mesh_large_df_sty_less_count_sort['Semantic Types Label'].iloc[:]\n",
    "y = mesh_large_df_sty_less_count_sort['mesh-count'].sort_values().iloc[:]\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title('Global \"Mesh-Semantic Type\" Zipf s law', loc='center',pad=10)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.ylabel('mesh number')\n",
    "plt.xlabel('semantic type rank', position=(0.5, 1.08))\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Density Rugged Plot\n",
    "data = mesh_large_df_sty_less_count_sort['mesh-count'].iloc[:]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.kdeplot(np.array(data), bw_method=0.5)\n",
    "sns.rugplot(np.array(data), color='r')\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.title('Mesh abundance for Semantic Type')\n",
    "#plt.yscale('log')\n",
    "print('Mesh abundance for Semantic Type:')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze MESH-STY-LITVAR (subset of MESH-STY.csv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('mesh_litvar_sty mesh:',mesh_litvar_sty['Preferred Label'].nunique())\n",
    "\n",
    "memory = mesh_litvar_sty.memory_usage().sum()\n",
    "print(f'The memory_usage of mesh_litvar_sty is {memory/ (1024 * 1024):.2f} MB.')\n",
    "\n",
    "file_size = os.path.getsize('ref-mesh-archive/MESH_STY_LITVAR1.csv')\n",
    "print(f'The size of MESH_STY_LITVAR1.csv is {file_size/ (1024 * 1024):.2f} MB.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sort Mesh-Sty-Litvar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#modulo groupby and bar\n",
    "mesh_litvar_sty_less = mesh_litvar_sty[['Preferred Label','Semantic Types Label']]\n",
    "\n",
    "### groupby.describe analysis by rsid--------------------\n",
    "mesh_litvar_sty_less_count = mesh_litvar_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "mesh_litvar_sty_less_count.columns = mesh_litvar_sty_less_count.columns.to_flat_index()\n",
    "new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "mesh_litvar_sty_less_count.columns = new_column_names\n",
    "\n",
    "mesh_litvar_sty_less_count_sort = mesh_litvar_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "print('MESH_STY_LITVAR1')\n",
    "mesh_litvar_sty_less_count_sort[['mesh-count','Semantic Types Label']]#.to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Graph Barh\n",
    "num = 20\n",
    "x = mesh_litvar_sty_less_count_sort['Semantic Types Label'].iloc[:num]\n",
    "y = mesh_litvar_sty_less_count_sort['mesh-count'].iloc[:num]\n",
    "plt.figure(figsize=(4, len(x)*0.25))\n",
    "plt.title('Litvar1 Mesh- Semantic Type enrichment', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "#plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "plt.ylabel('Semantic Types')\n",
    "plt.xlabel('mesh', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "# use log scale\n",
    "plt.gca().set_xscale('log')\n",
    "#plt.savefig('Reference Mesh- Semantic Type enrichment.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#'Global \"Mesh-Semantic Type\" Zipf s law'\n",
    "x = mesh_litvar_sty_less_count_sort['Semantic Types Label'].iloc[:]\n",
    "y = mesh_litvar_sty_less_count_sort['mesh-count'].sort_values().iloc[:]\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title('LitVar \"Mesh-Semantic Type\" Zipf s law', loc='center',pad=10)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.ylabel('mesh number')\n",
    "plt.xlabel('semantic type rank', position=(0.5, 1.08))\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# VISUALIZE DIFFERENCES\n",
    "\n",
    "#reload sorted df\n",
    "if 'mesh_large_df_sty_less_count_sort' in locals() and isinstance(mesh_large_df_sty_less_count_sort, pd.DataFrame):\n",
    "    pass\n",
    "else:\n",
    "    mesh_large_df_sty_less = mesh_large_df_sty[['Preferred Label','Semantic Types Label']]\n",
    "    mesh_large_df_sty_less_count = mesh_large_df_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    mesh_large_df_sty_less_count.columns = mesh_large_df_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    mesh_large_df_sty_less_count.columns = new_column_names\n",
    "    mesh_large_df_sty_less_count_sort = mesh_large_df_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "if 'mesh_litvar_sty_less_count_sort' in locals() and isinstance(mesh_litvar_sty_less_count_sort, pd.DataFrame):\n",
    "    pass\n",
    "else:\n",
    "    mesh_litvar_sty_less = mesh_litvar_sty[['Preferred Label','Semantic Types Label']]\n",
    "    mesh_litvar_sty_less_count = mesh_litvar_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    mesh_litvar_sty_less_count.columns = mesh_litvar_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    mesh_litvar_sty_less_count.columns = new_column_names\n",
    "    mesh_litvar_sty_less_count_sort = mesh_litvar_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "\n",
    "# create two sample dataframes\n",
    "df1 = mesh_large_df_sty_less_count_sort[['mesh-count', 'Semantic Types Label']]\n",
    "df2 = mesh_litvar_sty_less_count_sort[['mesh-count', 'Semantic Types Label']]\n",
    "\n",
    "# add a 'source' column to each dataframe\n",
    "df1['source'] = 'global'\n",
    "df2['source'] = 'litvar'\n",
    "\n",
    "# combine the dataframes\n",
    "combined_df = pd.merge(df1, df2, on=['Semantic Types Label'], how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# sort the dataframe by column 'A'\n",
    "#combined_df = combined_df.sort_values('mesh-count')\n",
    "\n",
    "# reset the index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# display the combined dataframe\n",
    "#combined_df['mesh-count_df2'] = combined_df['mesh-count_df2'].replace(np.nan, 0)\n",
    "combined_df = combined_df[-(combined_df['Semantic Types Label'] == 'Drug Delivery Device')]\n",
    "combined_df\n",
    "mesh_large_df_sty_less['Preferred Label'].nunique()\n",
    "combined_df = combined_df.sort_values(by='mesh-count_df2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# define a formatting function that generates a proportional bar\n",
    "def format_bar(value):\n",
    "    max_value = combined_df['mesh-count_df1'].max().max()  # get the maximum value in the dataframe\n",
    "    bar_width = int(value / max_value * 100)  # calculate the width of the bar as a percentage\n",
    "    return f'<div style=\"background-color: blue; width: {bar_width}%\">{value}</div>'\n",
    "\n",
    "def format_bar_2(value):\n",
    "    max_value = combined_df['mesh-count_df2'].max().max()  # get the maximum value in the dataframe\n",
    "    bar_width = int(value / max_value * 100)  # calculate the width of the bar as a percentage\n",
    "    return f'<div style=\"background-color: blue; width: {bar_width}%\">{value}</div>'\n",
    "\n",
    "\n",
    "# apply the formatting function to the dataframe\n",
    "df_formatted = combined_df.style.format({'mesh-count_df1': format_bar, 'mesh-count_df2': format_bar_2})\n",
    "\n",
    "# save the formatted dataframe to an HTML file\n",
    "with open('formatted_dataframe.html', 'w') as f:\n",
    "    f.write(df_formatted.render())\n",
    "\n",
    "# display the formatted dataframe\n",
    "df_formatted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####Define query variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "df.iloc[6]\n",
    "#df.loc['diet therapy']\n",
    "#find specific variable\n",
    "var1 = 'diet therapy'\n",
    "var2 = 'Diet, Food, and Nutrition'\n",
    "var3 = 'D004035'\n",
    "var4 = 'D000066888' #'Diet, Food, and Nutrition'\n",
    "var5 = 'Beverage'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "x = df[df.eq(var1).any(1)]\n",
    "y = df[df.eq(var2).any(1)]\n",
    "z = df[df.eq(var3).any(1)]\n",
    "k = df[df.eq(var4).any(1)]\n",
    "w = df[df.eq(var5).any(1)]\n",
    "\n",
    "list = [x,y,z]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reference Mesh list build:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build a 370 coherent and omic list of mesh term related to different topics\n",
    "- neurodegenerative diseases\n",
    "- skin diseases\n",
    "- infective diseases\n",
    "- reproductive phisyology\n",
    "- cancer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check avalable refs:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Check avalable refs:\n",
    "folder_path = \"ref-mesh-archive\"  # Replace with the actual folder path\n",
    "\n",
    "# Create a file path pattern to match CSV files\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "file_pattern\n",
    "\n",
    "# Use glob to get a list of file paths matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "csv_files_name = []\n",
    "# Print the list of CSV files\n",
    "for file in csv_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    csv_files_name.append(file_name)\n",
    "\n",
    "print('Available reference mesh lists:')\n",
    "csv_files_df = pd.Series(csv_files_name)\n",
    "ref_files_df = csv_files_df[csv_files_df.str.contains('ref_mesh_ng_')].reset_index(drop=True)\n",
    "ref_files_df_small = csv_files_df[csv_files_df.str.contains('small_ref_mesh_ng_')].reset_index(drop=True)\n",
    "ref_files_df_small"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_names =  [ 'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_',\n",
    "               'ref_' ]\n",
    "ref_names = [string + str(i) for i, string in enumerate(ref_names)]\n",
    "\n",
    "ref_list = []\n",
    "for name, ref, save  in zip(ref_names, ref_files_df, ref_files_df):\n",
    "    df = pd.read_csv('ref-mesh-archive/'+ref, index_col=0).reset_index(drop=True)\n",
    "    df = df.drop('Semantic Types Label', axis=1).drop_duplicates()\n",
    "    df = df.sort_values(by='mesh_id',ascending=True)\n",
    "    globals()[name] = df\n",
    "    df.to_csv('ref-mesh-archive/small_'+save)\n",
    "    ref_list.append(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create JSON reference\n",
    "https://jsoneditoronline.org/#left=local.lejobi&right=local.lejobi\n",
    "https://www.convertjson.com/json-to-html-table.htm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "folder_path = \"ref-mesh-archive\"\n",
    "\n",
    "\n",
    "num_elements = len(ref_files_df)  # Number of elements you want in the list\n",
    "value_list = []\n",
    "for i in range(1, num_elements + 1):\n",
    "    element = \"file_pat\" + str(i)\n",
    "    value_list.append(element)\n",
    "\n",
    "jdata_list_n = ['jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_',\n",
    "              'jdata_' ]\n",
    "jdata_list_n = [string + str(i) for i, string in enumerate(jdata_list_n)]\n",
    "\n",
    "jdata_list = []\n",
    "for name in jdata_list_n:\n",
    "    empty =[]\n",
    "    globals()[name] = empty\n",
    "    jdata_list.append(empty)\n",
    "\n",
    "\n",
    "file_patterns = []\n",
    "choose_ref = ref_files_df_small # or ref_files_df\n",
    "for i in choose_ref:\n",
    "    file_patterns.append(os.path.join(folder_path, i))\n",
    "\n",
    "for i, j  in zip(range(len(ref_files_df)), jdata_list):\n",
    "\n",
    "    with open(file_patterns[i], 'r') as file1:\n",
    "        csv_reader = csv.DictReader(file1)\n",
    "        # for row in csv_reader:\n",
    "        #     j_data1.append(row)\n",
    "        for row in csv_reader:\n",
    "            modified_row = {key: value for key, value in row.items() if key != ''} #drop col\n",
    "            j.append(modified_row)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#combined_data = {\n",
    "#    'data1': j_data1,\n",
    "#    'data2': j_data2 }\n",
    "\n",
    "combined_data = {}\n",
    "\n",
    "titles = [\n",
    "    \"Genaral Nutrition\",\n",
    "    \"Obesity, Weight Control and Compulsive Eating\",\n",
    "    \"Cardiovascular Health and Lipid Metabolism\",\n",
    "    \"Diabetes Mellitus Type II and Metabolic Syndrome\",\n",
    "    \"Vitamin and Micronutrients Metabolism and Deficiency-Related Diseases\",\n",
    "    \"Eating Behavior and Taste Sensation\",\n",
    "    \"Food Intolerances\",\n",
    "    \"Food Allergies\",\n",
    "    \"Diet-induced Oxidative Stress\",\n",
    "    \"Xenobiotics Metabolism\",\n",
    "]\n",
    "\n",
    "for tit, j_data in zip(titles, jdata_list):\n",
    "    key = tit\n",
    "    combined_data[key] = j_data\n",
    "\n",
    "#for i, j_data in enumerate(jdata_list, start=1):\n",
    "#    key = 'data' + str(i)\n",
    "#    combined_data[key] = j_data\n",
    "\n",
    "print(combined_data)\n",
    "#pd.DataFrame(combined_data)\n",
    "import pyperclip\n",
    "pyperclip.copy(str(combined_data))\n",
    "\n",
    "json_file = 'nutrigenetis_referencemesh_small.json'\n",
    "with open(json_file, 'w') as outfile:\n",
    "    json.dump(combined_data, outfile, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### add mesh id to every mesh list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add mesh id to every mesh list\n",
    "label = 'ng_nutri'\n",
    "dff = pd.read_csv('ref-mesh-archive/ref_mesh_'+label+'.csv',index_col=0).reset_index(drop=True)#.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "dff_id = pd.merge(dff, mesh_id_ref_df, left_on='Preferred Label', right_on='Preferred Label')\n",
    "dff_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dff_id.to_csv('ref-mesh-archive/ref_mesh_'+label+'.csv')\n",
    "#grpm_db_merge_id = grpm_db_merge_id.drop('Preferred Label', axis=1)\n",
    "check = pd.read_csv('ref-mesh-archive/ref_mesh_'+label+'.csv', index_col=0)\n",
    "check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "chack = check.drop('mesh_id_y',axis=1)\n",
    "check = check.rename(columns={'mesh_id_y': 'mesh_id'})\n",
    "check.to_csv('ref-mesh-archive/ref_mesh_'+label+'.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ref mesh cleaner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get rid from ref_mesh and run again\n",
    "xeno = ['Polymorphism, Genetic', 'DNA Methylation','Liver Neoplasms','Energy Metabolism','Metabolism, Inborn Errors','X-Ray Absorption Spectroscopy','Blood-Testis Barrier']\n",
    "oxi_stress = ['Food Deprivation','Food Analysis','Food, Formulated','Food Microbiology','Food Intolerance','Food Quality','Food Industry','Foods, Specialized','Food Chain','Foods, Specialized','Food Chain']\n",
    "intol = ['Depression','Osteoporosis','Immunotherapy','Anxiety','Anti-Inflammatory Agents, Non-Steroidal','Immunotherapy, Adoptive','Desensitization, Immunologic','Peanut Hypersensitivity','Milk Hypersensitivity', 'Egg Hypersensitivity','Immunotherapy, Active','Neurologic Manifestations' ,'Infectious Anemia Virus, Equine','Nut and Peanut Hypersensitivity' ,'Diarrhea Virus 1, Bovine Viral','Eye Movement Desensitization Reprocessing','Diarrhea Virus 2, Bovine Viral']\n",
    "eat_taste = ['Blood Glucose','Mouth Neoplasms','Glucose Transporter Type 1','Glucose Transporter Type 2','Auditory Perception','Citric Acid Cycle','Glucose Transporter Type 4','Loeys-Dietz Syndrome','United States Food and Drug Administration','Sodium-Glucose Transporter 2','Sodium-Glucose Transporter 2 Inhibitors','Glucose-6-Phosphate','Glucose Transporter Type 3','Pitch Perception','Depth Perception','Glucose-1-Phosphate Adenylyltransferase','Glucose Dehydrogenases','Glucosephosphates']\n",
    "cvd = ['DNA, Mitochondrial','Cell Differentiation','Protein Transport','Mitochondrial Proteins','Toll-Like Receptors','Genome, Mitochondrial','Genes, Mitochondrial','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Proton-Translocating ATPases','Mitochondrial Dynamics','Mitochondrial Membranes','Leukemia, Plasma Cell','Heart Neoplasms']\n",
    "dmt2_ms = [ 'MicroRNAs', 'Mitochondria', 'DNA, Mitochondrial', 'Mitochondrial Proteins', 'Pancreatic Neoplasms','Autophagy','Mitochondrial Diseases','Genome, Mitochondrial','Genes, Mitochondrial','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Dynamics','Mitochondrial Membranes','Mitochondrial Myopathies','RNA, Mitochondrial', 'Mitochondrial Encephalomyopathies','AIDS-Associated Nephropathy','Familial Primary Pulmonary Hypertension','Mitochondria, Heart','Autophagy-Related Protein-1 Homolog','Ocular Hypertension','Autophagy-Related Protein 7','Mitochondrial Trifunctional Protein','Mitochondrial Permeability Transition Pore','Mitochondrial Uncoupling Proteins','Mitochondrial ADP, ATP Translocases','Autophagy-Related Protein 8 Family','Mitochondrial Ribosomes','Mitochondrial Trifunctional Protein, alpha Subunit','Autophagy-Related Protein 12','Mitochondrial Turnover','Lysergic Acid Diethylamide','Mitochondrial Trifunctional Protein, beta Subunit','Chaperone-Mediated Autophagy','Mitochondrial Swelling','Mitochondrial Transmembrane Permeability-Driven Necrosis','Mitochondrial Size']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check mesh ref\n",
    "\n",
    "label = 'xeno'\n",
    "dff = pd.read_csv('ref-mesh-archive/ref_mesh_'+label+'.csv', index_col=0)\n",
    "dff['Preferred Label'].drop_duplicates()\n",
    "\n",
    "# get rid exact\n",
    "save_clean = True\n",
    "\n",
    "#clean mesh ref\n",
    "get_rid_list = xeno\n",
    "\n",
    "dff_rid = dff.copy()\n",
    "mask = dff['Preferred Label'].isin(get_rid_list)\n",
    "dff_rid = dff_rid[-mask]\n",
    "print(dff.describe())\n",
    "print(dff_rid.describe())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if save_clean == True:\n",
    "    dff_rid.to_csv('ref-mesh-archive/ref_mesh_'+label+'.csv')\n",
    "\n",
    "dff_rid['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dff['Preferred Label'].drop_duplicates()\n",
    "check = 'Fistula'\n",
    "dff_check = dff[dff['Preferred Label'].str.contains(check)]\n",
    "dff_check\n",
    "dff['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label = 'ob_bmi_less'\n",
    "dff = pd.read_csv('ref-mesh-archive/ref_mesh_'+label+'.csv', index_col=0)\n",
    "dff['Preferred Label'].drop_duplicates()\n",
    "\n",
    "\n",
    "# get rid contains\n",
    "save_clean = True\n",
    "tag = ''\n",
    "\n",
    "#clean mesh ref\n",
    "get_rid_list = ['Infant','Child','Adolescent','Elder','Maternal','Youth','Man','Woman','National','Neoplasm''Epidemiolo','Reproductive','Sexual','Genome','Animal','Doping','Social','Urban''Health C','Health E','Health F','Health I','Health P','Health S','Health T','ty Health','le Health','ic Health'\n",
    "]\n",
    "get_rid_list = ['Polymor','Genetic','Risk F']##['Fistula', 'Neoplasm','Labor', 'Chick']\n",
    "\n",
    "dff_rid = dff.copy()\n",
    "for get_rid in get_rid_list:\n",
    "    mask = dff['Preferred Label'].str.contains(get_rid)\n",
    "    dff_rid = dff_rid[-mask]\n",
    "\n",
    "if save_clean == True:\n",
    "    dff_rid.to_csv('ref-mesh-archive/ref_mesh_'+label+'_'+tag+'.csv')\n",
    "\n",
    "dff_rid['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dff['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build new ref_mesh lists"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mesh_litvar_sty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split nutritional topic into 8 different interest categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ngdb_df = pd.read_csv('NGDB1.csv', encoding='utf-8', sep=';')\n",
    "print('ngbd statistics:')\n",
    "print('caterories', ngdb_df.Category.nunique())\n",
    "print('genes', ngdb_df[' Gene'].nunique())\n",
    "print('SNPs', ngdb_df.SNPId.nunique())\n",
    "print('consequence type', ngdb_df['Gene_Consequence '].nunique())\n",
    "ngdb_df.Category.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General Ref-Mesh from ChatGPT input\n",
    "Per lista generata da ChatGPT: cerca in Preferred Labels and Synonyms corrisponding mesh entry\n",
    "    Two general classes of mesh of interest:\n",
    "    - 'topic' physiology\n",
    "    - 'topic' pathology\n",
    "    To be concatenated or searched separately?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Get ChatGPT - API\n",
    "\n",
    "# https://pub.towardsai.net/how-to-use-chatgpt-api-for-direct-interaction-from-colab-or-databricks-39969a0ead5f\n",
    "# https://platform.openai.com/account/api-keys\n",
    "# https://platform.openai.com/docs/api-reference/introduction\n",
    "\n",
    "\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "# please-paste-your-API-key-here\n",
    "\n",
    "def responseGPT(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}])\n",
    "    return print(completion)\n",
    "\n",
    "def chatWithGPT(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}])\n",
    "    return print(completion.choices[0].message.content)\n",
    "\n",
    "def fixMyCode(code):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"find error in my python script below and fix it: \" + code}])\n",
    "    return print(completion.choices[0].message.content)\n",
    "\n",
    "def createImageWithGPT(prompt):\n",
    "    completion = openai.Image.create(\n",
    "        prompt=prompt,\n",
    "        n=1,\n",
    "        size=\"512x512\"\n",
    "    )\n",
    "    return IPython.display.HTML(\"<img src =\" + completion.data[0].url + \">\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nutritional_topic = ['diseases and disorders realted to nutrition and diet ', 'diet, food consuption, eating behaviour and nutrition']\n",
    "infective_topic = ['infective agents, bacteria, virus and protozoan','infective diseases']\n",
    "reproductive_topic = ['reproductive system physiology','reproductive system pathology', 'Assisted reproductive technology']\n",
    "\n",
    "nutritional_topics = [\n",
    "    ['Obesity, overweight and body weight control', 'compulsive eating behavior'],\n",
    "    ['cardiovascular diseases','physiological processes realted to cardiovascular diseases','lipid metabolism in the context of cardiovascular diseases'],\n",
    "    ['Diabetes Melitus Type II and metabolic syndrome'],\n",
    "    ['Vitamin metabolism and Vitamins recommended intake levels','Micronutrients metabolism and Micronutrient recommended intake levels', 'disease related to vitamins and micronutrients deficiency'],\n",
    "    ['eating behaviour and taste sensation'],\n",
    "    ['food intolerances'],\n",
    "    ['food allergies'],\n",
    "    ['diet-induced oxidative stress'],\n",
    "    ['metabolism of xenobiotics'],\n",
    "]\n",
    "pd.Series(nutritional_topics)\n",
    "pd.Series(nutritional_topics).to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPT prompts\n",
    "\n",
    "# parameters--------------------------------------\n",
    "import pyperclip\n",
    "num_mesh = 120\n",
    "topics = nutritional_topics\n",
    "topic_id = 7\n",
    "#-----------------------------------------------\n",
    "topic_01  = topics[topic_id][0]\n",
    "topic_02  = topics[topic_id][1] if len(topics[topic_id])>=2 else None\n",
    "topic_03 = topics[topic_id][2] if len(topics[topic_id])>=3 else None\n",
    "\n",
    "prompt_01 = \"give me a comprehensive pyhton list of \"+str(num_mesh)+\" real Pubmed Mesh terms related to \"+ topic_01 +\":\\n gpt_01 = ['A','B','C',...]\\n\"\n",
    "prompt_02 = \"give me a comprehensive pyhton list of \"+str(num_mesh)+\" real Pubmed Mesh terms related to \"+ topic_02 +\":\\n gpt_02 = ['A','B','C',...]\\n\" if len(topics[topic_id])>=2 else None\n",
    "prompt_03 = \"give me a comprehensive pyhton list of \"+str(num_mesh)+\" real Pubmed Mesh terms related to \"+ topic_03 +\":\\n gpt_03 = ['A','B','C',...]\" if len(topics[topic_id])>=3 else None\n",
    "\n",
    "pyperclip.copy(prompt_01)\n",
    "pyperclip.copy(prompt_01+prompt_02) if len(topics[topic_id])>=2 else None\n",
    "pyperclip.copy(prompt_01+prompt_02+prompt_03) if len(topics[topic_id])>=3 else None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def getGPTterms(topic):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"give me a comprehensive list of \"+str(num_mesh)+\" real Pubmed Mesh terms related to \"+ topic +\" in pathon list format (list = ['A','B','C',...]):\\n gpt_01 = ['A','B','C',...]\\n\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion#.choices[0].message.content)\n",
    "\n",
    "response = getGPTterms(topic_01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### use GPT API\n",
    "persona = \"Socrates\"\n",
    "instruction = \"**instruction: you are \"+ persona +\", answer like you are \"+ persona +\"** \\n\"\n",
    "\n",
    "question = \"Tell me about the four elements?\"\n",
    "prompt = instruction +  question\n",
    "chatWithGPT(prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get gpt terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folder_path = \"ref-mesh-archive\"\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "csv_files = glob.glob(file_pattern)\n",
    "csv_files_name = []\n",
    "for file in csv_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    csv_files_name.append(file_name)\n",
    "\n",
    "print('Available gpt terms lists:')\n",
    "csv_files_df = pd.Series(csv_files_name)\n",
    "csv_files_df[csv_files_df.str.contains('gpt_')]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set parametes:--------------------------------\n",
    "\n",
    "topic_label = 'xeno' # <== CHANGE THIS! Overwrite risk!\n",
    "mother_df = mesh_litvar_sty\n",
    "overwrite_gpt = True\n",
    "\n",
    "#load existing gpt terms:\n",
    "load_saved_gpt = False\n",
    "topic_label = 'cvd' if load_saved_gpt == True else topic_label\n",
    "#-----------------------------------------------\n",
    "\n",
    "#ChatGPT list:\n",
    "gpt_01 = ['Xenobiotics',\n",
    " 'Metabolism',\n",
    " 'Biotransformation',\n",
    " 'Cytochrome P-450 Enzyme System',\n",
    " 'Phase I Metabolic Detoxication',\n",
    " 'Phase II Metabolic Detoxication',\n",
    " 'Oxidation-Reduction',\n",
    " 'Hydroxylation',\n",
    " 'Dealkylation',\n",
    " 'Deamination',\n",
    " 'Glucuronidation',\n",
    "          'Conjugation',\n",
    "          'Amination',\n",
    "          'Glycosylation',\n",
    " 'Sulfation',\n",
    " 'Methylation',\n",
    " 'Acetylation',\n",
    " 'Glutathione Transferase',\n",
    " 'Conjugation, Genetic',\n",
    " 'Pharmacokinetics',\n",
    " 'Absorption',\n",
    " 'Distribution',\n",
    " 'Excretion',\n",
    " 'Half-Life',\n",
    " 'Clearance',\n",
    " 'Bioavailability',\n",
    " 'Blood-Brain Barrier',\n",
    " 'Blood-Air Barrier',\n",
    " 'Blood-Retinal Barrier',\n",
    " 'Blood-Testis Barrier',\n",
    " 'Placental Barrier',\n",
    " 'Biological Transport',\n",
    " 'Active Transport, Cell Nucleus',\n",
    " 'Facilitated Diffusion',\n",
    " 'Endocytosis',\n",
    " 'Exocytosis',\n",
    " 'Transcytosis',\n",
    "          \"Xenobiotics\",\n",
    "          \"Drug Metabolism\",\n",
    "          \"Cytochrome P-450 Enzyme System\",\n",
    "          \"Phase I Metabolic Detoxication\",\n",
    "          \"Phase II Metabolic Detoxication\",\n",
    "          \"Toxicokinetics\",\n",
    "          \"Biotransformation\",\n",
    "          \"Glutathione Transferase\",\n",
    "          \"Oxidoreductases\",\n",
    "          \"Drug-Related Side Effects and Adverse Reactions\",\n",
    "          \"Enzyme Induction\",\n",
    "          \"Pharmacokinetics\",\n",
    "          \"Enzyme Inhibitors\",\n",
    "          \"Chemical and Drug Induced Liver Injury\",\n",
    "          \"Drug Interactions\",\n",
    "          \"Liver\",\n",
    "          \"Toxicity Tests\",\n",
    "          \"Pharmaceutical Preparations\",\n",
    "          \"Metabolomics\",\n",
    "          \"Drug Evaluation, Preclinical\",\n",
    "          \"Liver Microsomes\",\n",
    "          \"Liver Neoplasms, Experimental\",\n",
    "          \"Drug Discovery\",\n",
    "          \"Polymorphism, Genetic\",\n",
    "          \"Drug Resistance\",\n",
    "          \"Toxicity\",\n",
    "          \"Hepatocytes\",\n",
    "          \"Carboxylesterase\",\n",
    "          \"Aryl Hydrocarbon Hydroxylases\",\n",
    "          \"Mixed Function Oxygenases\",\n",
    "          \"Uridine Diphosphate Glucuronic Acid\",\n",
    "          \"Pharmacogenetics\",\n",
    "          \"Drug-Induced Liver Injury\",\n",
    "          \"Pharmaceutical Preparations\",\n",
    "          \"Glutathione\",\n",
    "          \"Metabolome\",\n",
    "          \"Toxicokinetics\",\n",
    "          \"Receptors, Aryl Hydrocarbon\",\n",
    "          \"P-Glycoprotein\",\n",
    "          \"Hepatocytes\",\n",
    "          \"Drug Resistance\",\n",
    "          \"Drug Interactions\",\n",
    "          \"Cytochrome P-450\",\n",
    "          \"CYP1A2\",\n",
    "          \"Drug Metabolism, Phase I\",\n",
    "          \"Drug Metabolism, Phase II\",\n",
    "          \"Oxidative Stress\",\n",
    "          \"Pharmacokinetics\",\n",
    "          \"Uridine Diphosphate Glucuronic Acid\",\n",
    "          \"Liver\",\n",
    "          \"Enzyme Induction\",\n",
    "          \"Drug-Related Side Effects and Adverse Reactions\",\n",
    "          \"Pharmacogenetics\",\n",
    "          \"Liver Diseases\",\n",
    "          \"Pharmacogenomics\",\n",
    "          \"Receptors, Drug\",\n",
    "          \"Liver Neoplasms, Experimental\",\n",
    "          \"Liver Microsomes\",\n",
    "          \"Drug Evaluation, Preclinical\",\n",
    "          \"Drug Resistance\",\n",
    "          \"Liver Cirrhosis, Experimental\",\n",
    "          \"Liver Failure\",\n",
    "          \"Liver Function Tests\",\n",
    "          \"Liver Regeneration\",\n",
    "          \"Toxicology\",\n",
    "          \"Toxicity Tests\",\n",
    "          \"Drug Design\",\n",
    "          \"Drug Screening Assays, Antitumor\",\n",
    "          \"Liver Diseases, Alcoholic\",\n",
    "          \"Liver Transplantation\",\n",
    "          \"Toxicity\",\n",
    "          \"Toxicology\",\n",
    "          \"Pharmacy Services\",\n",
    "          \"Polypharmacy\",\n",
    "          \"Toxicology\",\n",
    "          \"Toxicogenetics\",\n",
    "          \"Toxicology\",\n",
    "          \"Toxicology, Environmental\",\n",
    "          \"Toxicology, Forensic\",\n",
    "          \"Toxicology, Legal\",\n",
    "          \"Toxicology, Pharmaceutical\",\n",
    "          \"Toxicology, Veterinary\",\n",
    "          \"Toxicology Tests\",\n",
    "          \"Toxicology, Clinical\",\n",
    "          \"Toxicity Tests\",\n",
    "          \"Toxicity Tests, Acute\",\n",
    "          \"Toxicity Tests, Chronic\",\n",
    "          \"Toxicity Tests, Subacute\",\n",
    "          \"Toxicity Tests, Subchronic\",\n",
    "          \"Toxicity Tests, Sublethal\",\n",
    "          \"Toxicity Tests, Mutagenicity\",\n",
    "          \"Toxicity Tests, Reproduction\",\n",
    "          \"Toxicity Tests, Teratogenicity\",\n",
    "          \"Xenobiotics\",\n",
    "          \"Environmental Pollutants\",\n",
    "          \"Water Pollutants, Chemical\",\n",
    "          \"Soil Pollutants\",\n",
    "          \"Air Pollutants\",\n",
    "          \"Pesticides\",\n",
    "          \"Pharmaceutical Preparations\",\n",
    "          \"Cosmetics\",\n",
    "          \"Food Additives\",\n",
    "          \"Radiation, Ionizing\",\n",
    "          \"Radiation, Nonionizing\",\n",
    "          \"Toxicity\",\n",
    "          \"Toxicology\",\n",
    "          \"Toxicology, Environmental\",\n",
    "          \"Toxicology, Forensic\",\n",
    "          \"Toxicology, Legal\",\n",
    "          \"Toxicology, Pharmaceutical\",\n",
    "          \"Toxicology, Veterinary\",\n",
    "          \"Pharmacology\",\n",
    "          \"Pharmacology, Clinical\",\n",
    "          \"Pharmacology, Experimental\",\n",
    "          \"Pharmacy\",\n",
    "          \"Pharmacy Service, Hospital\",\n",
    "          \"Pharmacy Services\",\n",
    "          \"Polypharmacy\",\n",
    "          \"Toxicogenetics\",\n",
    "          \"Drug-Related Side Effects and Adverse Reactions\",\n",
    "          \"Chemical and Drug Induced Liver Injury\",\n",
    "          \"Liver Diseases\",\n",
    "          \"Liver Cirrhosis, Experimental\",\n",
    "          \"Liver Failure\",\n",
    "          \"Liver Function Tests\",\n",
    "          \"Liver Neoplasms, Experimental\",\n",
    "          \"Liver Regeneration\",\n",
    "          \"Liver Transplantation\",\n",
    "          \"Carboxylesterase\",\n",
    "          \"Aryl Hydrocarbon Hydroxylases\",\n",
    "          \"Mixed Function Oxygenases\",\n",
    "          \"Glutathione Transferase\",\n",
    "          \"Receptors, Aryl Hydrocarbon\",\n",
    "          \"P-Glycoprotein\",\n",
    "          \"Cytochrome P-450 CYP1A2\",\n",
    "          \"Oxidative Stress\",\n",
    "          \"Drug Metabolism, Phase I\",\n",
    "          \"Drug Metabolism, Phase II\",\n",
    "          \"Pharmacokinetics\",\n",
    "          \"Pharmacogenetics\",\n",
    "          \"Pharmacogenomics\"\n",
    "          ]\n",
    "#gpt_01 = []\n",
    "gpt_02 =  []\n",
    "#gpt_02 = []\n",
    "gpt_03  = []# Paste here\n",
    "#n.b. remember to remove  general terms like \"Proteins\" to avoid bias (get_rid section)\n",
    "\n",
    "gpt_all_ser = pd.DataFrame()\n",
    "if load_saved_gpt == True:\n",
    "    if os.path.exists('ref-mesh-archive/gpt_terms_'+topic_label+'.csv'):\n",
    "        gpt_all_ser_df = pd.read_csv('ref-mesh-archive/gpt_terms_'+topic_label+'.csv')\n",
    "        gpt_all_ser = gpt_all_ser_df['gpt_terms'].drop_duplicates()\n",
    "        print('gpt terms loaded')\n",
    "    else:\n",
    "        print('no gpt terms to load')\n",
    "else:\n",
    "    gpt_01_df = pd.DataFrame({'gpt_terms': gpt_01})\n",
    "    gpt_01_df['series'] = 'gpt_01'\n",
    "    gpt_all_ser_df = gpt_01_df\n",
    "    if len(gpt_02) > 0 :\n",
    "        print('gpt_02 included')\n",
    "        gpt_02_df = pd.DataFrame({'gpt_terms': gpt_02})\n",
    "        gpt_02_df['series'] = 'gpt_02'\n",
    "        gpt_all_ser_df = pd.concat([gpt_all_ser_df, gpt_02_df], axis=0)\n",
    "    if len(gpt_03) > 0 :\n",
    "        print('gpt_03 included')\n",
    "        gpt_03_df = pd.DataFrame({'gpt_terms': gpt_03})\n",
    "        gpt_03_df['series'] = 'gpt_03'\n",
    "        gpt_all_ser_df = pd.concat([gpt_all_ser_df, gpt_03_df], axis=0)\n",
    "\n",
    "#get rid of unwanted gpt terms: Exact match\n",
    "print('raw', len(gpt_all_ser_df))\n",
    "get_rid = ['Proteins','Protein','DNA','RNA',\n",
    "           'Polymor','Genetic',\n",
    "           'Infant',\n",
    "           'Child',\n",
    "           'Adolescent',\n",
    "           'Elder',\n",
    "           'Maternal',\n",
    "           'Youth',\n",
    "           'Man',\n",
    "           'Woman',\n",
    "           'National',\n",
    "           'Neoplasm',\n",
    "           'Reproductive',\n",
    "           'Sexual',\n",
    "           'Genome',\n",
    "           'Animal',\n",
    "           'Doping',\n",
    "           'Social',\n",
    "           'Urban',\n",
    "           ] # generic terms creating bias\n",
    "# attetion to 'Mitochondrial'\n",
    "mask = gpt_all_ser_df['gpt_terms'].isin(get_rid)\n",
    "gpt_all_ser_df = gpt_all_ser_df[-mask]\n",
    "print('clean I:',len(gpt_all_ser_df))\n",
    "\n",
    "#get rid of unwanted gpt terms: Contained str\n",
    "get_rid_gpt = True\n",
    "get_rid_spec = ['Pharma',\n",
    "                'Cosme',\n",
    "                'Drug',\n",
    "                'Distribution'\n",
    "                ]\n",
    "if get_rid_gpt == True:\n",
    "    for rid in get_rid_spec:\n",
    "        mask = gpt_all_ser_df['gpt_terms'].str.contains(rid)#.isin(get_rid_spec)\n",
    "        gpt_all_ser_df = gpt_all_ser_df[-mask]\n",
    "    print('clean II:',len(gpt_all_ser_df))\n",
    "\n",
    "print('drop dup:',gpt_all_ser_df.gpt_terms.nunique())\n",
    "\n",
    "if overwrite_gpt == True:\n",
    "    gpt_all_ser_df.to_csv('ref-mesh-archive/gpt_terms_'+topic_label+'.csv')\n",
    "    print('gpt terms saved')\n",
    "\n",
    "gpt_all_ser = gpt_all_ser_df['gpt_terms'].drop_duplicates()\n",
    "pyperclip.copy(str(gpt_all_ser.to_list()))\n",
    "gpt_all_ser_df\n",
    "#pyperclip.copy(str(gpt_all_ser.to_list()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get rid of gpt terms LOG\n",
    "on nutri: [ 'Immune System',    'Nail Concentrations',    'Natural Killer Cells',    'Nerve Function',    'PYY3-36',    'Phenotype',    'Pregnancy',    'Single Nucleotide Polymorphisms',    'Transplantation',    'T-Lymphocytes',    'Dendritic Cells',    'Macrophages',    'Graft',    'Phagocytosis',    'Calcium',    'Iron',    'Phosphorus',    'Copper',    'Development',    'Developmental',    'Gene Expression']\n",
    "on ob_bmi: ['Transcriptome',\n",
    "'RNA, Ribosomal, 16S','Proteomics','Prevalence','Epidemiologic Studies','Epidemiology','Epidemics','Sick Role']\n",
    "oxi_stress. ['Chemokines','Cytokines','Diet','Interleukin']\n",
    "xeno: ['Pharma','Cosme','Drug','Distribution']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### serch gpt terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use gpt terms to filter mother_df and Get Mesh:\n",
    "overwrite_ref = True\n",
    "\n",
    "split_01_02 = False\n",
    "if split_01_02 == True:\n",
    "    print(topics[topic_id][0],'topic')\n",
    "    #get mesh\n",
    "    chat_mesh_01 = pd.DataFrame(columns=mother_df.columns) #Empty DataFrame, same structure\n",
    "    for i in gpt_01:\n",
    "        meshy = mother_df[mother_df['Preferred Label'].str.contains(i).fillna(False)]\n",
    "        chat_mesh_01 = pd.concat([meshy,chat_mesh_01])\n",
    "    print(chat_mesh_01['Preferred Label'].nunique())\n",
    "\n",
    "    print(topics[topic_id][1],'topic')\n",
    "    #get mesh\n",
    "    chat_mesh_02 = pd.DataFrame(columns=mother_df.columns) #Empty DataFrame, same structure\n",
    "    for i in gpt_02:\n",
    "        meshy = mother_df[mother_df['Preferred Label'].str.contains(i).fillna(False)]\n",
    "        chat_mesh_02 = pd.concat([meshy,chat_mesh_02])\n",
    "    print(chat_mesh_02['Preferred Label'].nunique())\n",
    "    if 1 >2 :\n",
    "        #analyze semantics\n",
    "        chat_mesh_01_grup = chat_mesh_01.groupby('Semantic Types Label').describe()\n",
    "        chat_mesh_01_sem = chat_mesh_01_grup.index.to_list()\n",
    "        chat_mesh_02_grup = chat_mesh_02.groupby('Semantic Types Label').describe()\n",
    "        chat_mesh_02_sem = chat_mesh_02_grup.index.to_list()\n",
    "        #get sum of semantic types\n",
    "        chat_mesh_sem_sum = chat_mesh_02_sem + chat_mesh_01_sem\n",
    "        chat_mesh_sem_sum_ser = pd.Series(chat_mesh_sem_sum)\n",
    "        print('chat_mesh_sem_sum_ser',chat_mesh_sem_sum_ser.nunique())\n",
    "\n",
    "#get mesh\n",
    "chat_mesh_all = pd.DataFrame(columns=mother_df.columns) #Empty DataFrame, same structure\n",
    "time1 =datetime.now()\n",
    "for i in gpt_all_ser:\n",
    "    #meshy = mother_df[mother_df['Preferred Label'].str.contains(i).fillna(False)]\n",
    "    meshy = mother_df[mother_df['Preferred Label'].str.extract(f'({i})').notna().any(axis=1)]\n",
    "    chat_mesh_all = pd.concat([meshy,chat_mesh_all])\n",
    "time2 =datetime.now()\n",
    "print(time2-time1)\n",
    "\n",
    "#analyze semantics\n",
    "chat_mesh_all_grup = chat_mesh_all.groupby('Semantic Types Label').describe()\n",
    "chat_mesh_all_grup_sem = chat_mesh_all_grup.index.to_list()\n",
    "\n",
    "new_name = 'chat_mesh_'+topic_label\n",
    "exec(f'{new_name} = chat_mesh_all')\n",
    "print(new_name, 'created')\n",
    "print('   gpt terms:', gpt_all_ser.nunique())\n",
    "print('   mesh found', chat_mesh_all['Preferred Label'].nunique())\n",
    "print('   semantic groups',chat_mesh_all['Semantic Types Label'].nunique())\n",
    "\n",
    "# GET ref mesh list\n",
    "if os.path.exists('ref-mesh-archive/ref_mesh_'+topic_label+'.csv'):\n",
    "    print('ref_mesh_'+topic_label+'.csv alreday exists')\n",
    "    if overwrite_ref == True:\n",
    "        globals()[new_name][['Preferred Label', 'Semantic Types Label']].to_csv('ref-mesh-archive/ref_mesh_'+topic_label+'.csv')\n",
    "        print('overwritten')\n",
    "else:\n",
    "    globals()[new_name][['Preferred Label', 'Semantic Types Label']].to_csv('ref-mesh-archive/ref_mesh_'+topic_label+'.csv')\n",
    "    print('ref_mesh_'+topic_label+'.csv saved')\n",
    "\n",
    "chat_mesh_all[['Preferred Label','mesh_id','Semantic Types Label']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#compare\n",
    "print(len(pd.read_csv('ref-mesh-archive/ref_mesh_cvd.csv')['Preferred Label'].drop_duplicates()))\n",
    "print(len(pd.read_csv('ref-mesh-archive/ref_mesh_cvd_lipo.csv')['Preferred Label'].drop_duplicates()))\n",
    "gpt_all_ser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### clean output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check mesh ref\n",
    "label = topic_label\n",
    "dff = pd.read_csv('ref-mesh-archive/ref_mesh_'+label+'.csv')\n",
    "dff['Preferred Label'].drop_duplicates()\n",
    "check = 'Apop'\n",
    "dff_check = dff[dff['Preferred Label'].str.contains(check)]\n",
    "dff['Preferred Label'].drop_duplicates()\n",
    "dff_check['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_clean = True\n",
    "\n",
    "#clean mesh ref\n",
    "get_rid_list = ['oma','Apop','Cell','gen Ty','Cilia','Cytokinesis','DNA','Diabet','Fluorescein','Intercellular Signaling','Leukem','Lysergic Acid','Loeys-Dietz','Mitochondrial','Peptide Hormones','Toll-Like','Tumor']\n",
    "dff_rid = dff.copy()\n",
    "for get_rid in get_rid_list:\n",
    "    mask = dff['Preferred Label'].str.contains(get_rid)\n",
    "    dff_rid = dff_rid[-mask]\n",
    "\n",
    "if save_clean == True:\n",
    "    dff_rid.to_csv('ref-mesh-archive/ref_mesh_'+label+'.csv')\n",
    "\n",
    "dff_rid['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "get rid log\n",
    "ob_bmi mesh ['Asthma','Cardio','Child H','Chromosome','Delivery','Diethyl','Drug','Employer','Electronic','Epigenomics','Ocular','Pulmon','Malignant','Organizat','Person','Occupat','Literacy','Level Se','h Prop','h Reso','es Res','for the Age','Indigen','h Syst','Deter','Antibod','Insulin-Like Growth Factor','Express','Patient','Pregnancy Complications','Rural','Sleep','Snow','Veter','Volunt','Water']\n",
    "\n",
    "cvd_list = ['oma','Apop','Cell','gen Ty','Cilia','Cytokinesis','DNA','Diabet','Fluorescein','Intercellular Signaling','Leukem','Lysergic Acid','Loeys-Dietz','Mitochondrial','Peptide Hormones','Toll-Like','Tumor']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ref_mesh semantic type ripper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Sty groups\n",
    "chat_mesh_all_grup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(gpt_nutri)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Rip form complete ref mesh\n",
    "save = False\n",
    "rip_list = ['Cell',\n",
    "            'Receptor',\n",
    "            'Hormone',\n",
    "            'Tissue',\n",
    "            'Body Part, Organ, or Organ Component',\n",
    "            'Congenital Abnormality',\n",
    "            'Anatomical Abnormality',\n",
    "            'Indicator, Reagent, or Diagnostic Aid']\n",
    "rip_list += ['Neoplastic Process']\n",
    "rip_list\n",
    "\n",
    "#remove_value = chat_mesh_all[chat_mesh_all['Semantic Types Label'].isin(rip_list)]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "#count analyzer\n",
    "#remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count']> 56]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "\n",
    "mask = chat_mesh_all['Semantic Types Label'].isin(rip_list)\n",
    "chat_mesh_all_redux = chat_mesh_all[-mask]\n",
    "\n",
    "if save == True:\n",
    "    chat_mesh_all_redux[['Preferred Label', 'Semantic Types Label']].to_csv('ref-mesh-archive/ref_mesh_'+topic_label+'_redux.csv')\n",
    "chat_mesh_all_redux['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#STY Ripper LOG:\n",
    "rip_list = ['Cell',\n",
    "            'Receptor',\n",
    "            'Hormone',\n",
    "            'Tissue',\n",
    "            'Body Part, Organ, or Organ Component',\n",
    "            'Congenital Abnormality',\n",
    "            'Anatomical Abnormality',\n",
    "            'Indicator, Reagent, or Diagnostic Aid']\n",
    "rip_list += ['Neoplastic Process']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### build ref mesh sterp by step\n",
    "#### neuro trial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#ChatGPT list:\n",
    "#give me a comprehensive pyhton list (list= ['A','B','C',...]) of 100 real Mesh terms related to nervous system physiology\n",
    "#Now continue the  list with another 100 different Mesh tems in python list format.\n",
    "gpt_neuro_phys = [\n",
    "    'Action Potentials',\n",
    "    'Adrenoleukodystrophy',\n",
    "    'Alzheimer Disease',\n",
    "    'Amyloid beta-Peptides',\n",
    "    'Amyloid beta-Protein Precursor',\n",
    "    'Amyloidosis',\n",
    "    'Amyotrophic Lateral Sclerosis',\n",
    "    'Ataxia',\n",
    "    'Axons',\n",
    "    'Basal Ganglia',\n",
    "    'Blood-Brain Barrier',\n",
    "    'Brain',\n",
    "    'Brain Injuries',\n",
    "    'Brain-Derived Neurotrophic Factor',\n",
    "    'Calcium Signaling',\n",
    "    'Cerebellum',\n",
    "    'Cerebral Amyloid Angiopathy',\n",
    "    'Cerebral Cortex',\n",
    "    'Charcot-Marie-Tooth Disease',\n",
    "    'Cholinergic Neurons',\n",
    "    'Cognition',\n",
    "    'Copper',\n",
    "    'Cortical Spreading Depression',\n",
    "    'Creutzfeldt-Jakob Syndrome',\n",
    "    'Dementia',\n",
    "    'Dementia with Lewy Bodies',\n",
    "    'Dentate Gyrus',\n",
    "    'Depression',\n",
    "    'Dopamine',\n",
    "    'Dopaminergic Neurons',\n",
    "    'Electrophysiology',\n",
    "    'Encephalitis',\n",
    "    'Epilepsy',\n",
    "    'Excitatory Amino Acid Antagonists',\n",
    "    'Excitatory Postsynaptic Potentials',\n",
    "    'GABAergic Neurons',\n",
    "    'Gangliosidoses',\n",
    "    'Glial Fibrillary Acidic Protein',\n",
    "    'Gliosis',\n",
    "    'Glycine Plasma Membrane Transport Proteins',\n",
    "    'Hippocampus',\n",
    "    'Huntington Disease',\n",
    "    'Intracellular Signaling Peptides and Proteins',\n",
    "    'Ion Channels',\n",
    "    'Ischemia',\n",
    "    'Limbic System',\n",
    "    'Lipid Metabolism',\n",
    "    'Long-Term Potentiation',\n",
    "    'Manganese Poisoning',\n",
    "    'Membrane Potentials',\n",
    "    'Membrane Proteins',\n",
    "    'Memory',\n",
    "    'Mental Disorders',\n",
    "    'Microglia',\n",
    "    'Mitochondria',\n",
    "    'Molecular Chaperones',\n",
    "    'Molecular Motor Proteins',\n",
    "    'Motor Neurons',\n",
    "    'Multiple Sclerosis',\n",
    "    'Muscular Atrophy, Spinal',\n",
    "    'Myelin Sheath',\n",
    "    'Nerve Degeneration',\n",
    "    'Nerve Growth Factors',\n",
    "    'Nerve Regeneration',\n",
    "    'Nerve Tissue Proteins',\n",
    "    'Nervous System Diseases',\n",
    "    'Neural Inhibition',\n",
    "    'Neural Pathways',\n",
    "    'Neurofibrillary Tangles',\n",
    "    'Neurogenesis',\n",
    "    'Neuroglia',\n",
    "    'Neuronal Plasticity',\n",
    "    'Neurons',\n",
    "    'Neuroprotective Agents',\n",
    "    'Neurosecretion',\n",
    "    'Neurotransmitter Agents',\n",
    "    'Nicotinic Receptors',\n",
    "    'Nitric Oxide',\n",
    "    'Nuclear Proteins',\n",
    "    'Oligodendroglia',\n",
    "    'Oxidative Stress',\n",
    "    'Parkinson Disease',\n",
    "    'Phosphatidylinositol 3-Kinases',\n",
    "    'Phosphorylation',\n",
    "    'Pituitary-Adrenal System',\n",
    "    'Presenilin-1',\n",
    "    'Presynaptic Terminals',\n",
    "    'Protein Aggregates',\n",
    "    'Protein Folding',\n",
    "    'Protein Transport',\n",
    "    'Protein-Serine-Threonine Kinases',\n",
    "    'Protein-Tyrosine Kinases',\n",
    "    'Receptor, Adenosine A2A',\n",
    "    'Receptor, Muscarinic M1',\n",
    "    'Receptor, Nerve Growth Factor',\n",
    "    'Receptors, AMPA',\n",
    "    'Receptors, Dopamine',\n",
    "    'Receptors, Glutamate',\n",
    "    'Receptors, GABA-A',\n",
    "    'Receptors, GABA-B',\n",
    "    'Receptors, Metabotropic Glutamate',\n",
    "    'Receptors, N-Methyl-D-Aspartate',\n",
    "    'Receptors, Nicotinic',\n",
    "    'Receptors, Serotonin',\n",
    "    'Hippocampus',\n",
    "    'Amygdala',\n",
    "    'Cerebellum',\n",
    "    'Brain Stem',\n",
    "    'Substantia Nigra',\n",
    "    'Basal Ganglia',\n",
    "    'Corpus Striatum',\n",
    "    'Thalamus',\n",
    "    'Hypothalamus',\n",
    "    'Limbic System',\n",
    "    'Synapse',\n",
    "    'Neurotransmitter',\n",
    "    'Excitatory Postsynaptic Potential',\n",
    "    'Inhibitory Postsynaptic Potential',\n",
    "    'Action Potential',\n",
    "    'Ion Channels',\n",
    "    'Glutamate',\n",
    "    'GABA',\n",
    "    'Dopamine',\n",
    "    'Serotonin',\n",
    "    'Acetylcholine',\n",
    "    'Norepinephrine',\n",
    "    'Epinephrine',\n",
    "    'Histamine',\n",
    "    'Neuropeptides',\n",
    "    'Neuropeptide Y',\n",
    "    'Substance P',\n",
    "    'Cholecystokinin',\n",
    "    'Enkephalins',\n",
    "    'Beta-endorphins',\n",
    "    'Neuronal Plasticity',\n",
    "    'Long-Term Potentiation',\n",
    "    'Long-Term Depression',\n",
    "    'Neurogenesis',\n",
    "    'Neuronal Migration',\n",
    "    'Neuronal Differentiation',\n",
    "    'Axon Guidance',\n",
    "    'Neuronal Survival',\n",
    "    'Neural Circuits',\n",
    "    'Neural Networks',\n",
    "    'Neural Coding',\n",
    "    'Central Pattern Generators',\n",
    "    'Neuronal Oscillations',\n",
    "    'Brain Waves',\n",
    "    'Gamma Waves',\n",
    "    'Theta Waves',\n",
    "    'Delta Waves',\n",
    "    'Alpha Waves',\n",
    "    'Beta Waves',\n",
    "    'Sleep',\n",
    "    'REM Sleep',\n",
    "    'Non-REM Sleep',\n",
    "    'Sleep Disorders',\n",
    "    'Insomnia',\n",
    "    'Sleep Apnea',\n",
    "    'Narcolepsy',\n",
    "    'Cataplexy',\n",
    "    'Cataplexy',\n",
    "    'Restless Legs Syndrome',\n",
    "    'Parkinsonism',\n",
    "    'Parkinsonian Tremor',\n",
    "    'Parkinsonian Rigidity',\n",
    "    'Parkinsonian Bradykinesia',\n",
    "    'Parkinsonian Akinesia',\n",
    "    'Parkinsonian Gait',\n",
    "    'Parkinsonian Posture',\n",
    "    'Parkinsonism, Secondary',\n",
    "    'Huntington Disease',\n",
    "    'Choreiform Disorders',\n",
    "    'Chorea',\n",
    "    'Athetosis',\n",
    "    'Dystonia',\n",
    "    'Myoclonus',\n",
    "    'Tics',\n",
    "    'Tourette Syndrome',\n",
    "    'Friedreich Ataxia',\n",
    "    'Spinocerebellar Ataxias',\n",
    "    'Cerebellar Ataxia',\n",
    "    'Ataxia Telangiectasia',\n",
    "    'Restless Legs Syndrome',\n",
    "    'Periodic Limb Movements',\n",
    "    'Peripheral Neuropathies',\n",
    "    'Autonomic Neuropathy',\n",
    "    'Diabetic Neuropathies',\n",
    "    'Charcot-Marie-Tooth Disease',\n",
    "    'Guillain-Barre Syndrome',\n",
    "    'Chronic Inflammatory Demyelinating Polyneuropathy',\n",
    "    'Multiple Mononeuropathy',\n",
    "    'Mononeuritis Multiplex',\n",
    "    'Carpal Tunnel Syndrome',\n",
    "    'Brachial Plexus Neuropathies',\n",
    "    'Amyotrophic Lateral Sclerosis',\n",
    "    'Spinal Muscular Atrophy',\n",
    "    'Progressive Bulbar Palsy',\n",
    "    'Progressive Supranuclear Palsy',\n",
    "    'Corticobasal Degeneration',\n",
    "    'Multiple System Atrophy',\n",
    "    'Lewy Body Disease',\n",
    "    'Frontotemporal Dementia',\n",
    "    'Alzheimer Disease',\n",
    "    'Vascular Dementia']\n",
    "\n",
    "#give me a comprehensive pyhton list (list= ['A','B','C',...]) of 100 real Mesh terms related to nervous neudegenarative diseases and neurologic pathology.\n",
    "#Now continue the  list with another 100 different Mesh tems in python list format .\n",
    "\n",
    "gpt_neuro_path = ['Alzheimer Disease',\n",
    "                  'Amyotrophic Lateral Sclerosis',\n",
    "                  'Ataxia',\n",
    "                  'Basal Ganglia Diseases',\n",
    "                  'Brain Diseases',\n",
    "                  'Brain Infarction',\n",
    "                  'Brain Injury, Chronic',\n",
    "                  'Cauda Equina Syndrome',\n",
    "                  'Central Nervous System Diseases',\n",
    "                  'Cerebellar Diseases',\n",
    "                  'Cerebral Amyloid Angiopathy',\n",
    "                  'Cerebral Hemorrhage',\n",
    "                  'Cerebral Palsy',\n",
    "                  'Cerebrovascular Disorders',\n",
    "                  'Charcot-Marie-Tooth Disease',\n",
    "                  'Chorea',\n",
    "                  'Cortical Blindness',\n",
    "                  'Creutzfeldt-Jakob Syndrome',\n",
    "                  'Dementia',\n",
    "                  'Diffuse Axonal Injury',\n",
    "                  'Dystonia',\n",
    "                  'Encephalitis',\n",
    "                  'Encephalomyelitis',\n",
    "                  'Epilepsy',\n",
    "                  'Essential Tremor',\n",
    "                  'Friedreich Ataxia',\n",
    "                  'Gait Disorders, Neurologic',\n",
    "                  'Gerstmann-Straussler-Scheinker Disease',\n",
    "                  'Hallervorden-Spatz Syndrome',\n",
    "                  'Hemiplegia',\n",
    "                  'Hereditary Sensory and Autonomic Neuropathies',\n",
    "                  'Huntington Disease',\n",
    "                  'Hydrocephalus',\n",
    "                  'Intracranial Aneurysm',\n",
    "                  'Intracranial Arteriovenous Malformations',\n",
    "                  'Intracranial Embolism and Thrombosis',\n",
    "                  'Intracranial Hemorrhages',\n",
    "                  'Intracranial Hypertension',\n",
    "                  'Kuru',\n",
    "                  'Leigh Disease',\n",
    "                  'Leukoencephalopathies',\n",
    "                  'Lewy Body Disease',\n",
    "                  'Locked-In Syndrome',\n",
    "                  'Meningitis',\n",
    "                  'Meningoencephalitis',\n",
    "                  'Migraine Disorders',\n",
    "                  'Mild Cognitive Impairment',\n",
    "                  'Motor Neuron Disease',\n",
    "                  'Multiple Sclerosis',\n",
    "                  'Muscle Spasticity',\n",
    "                  'Muscular Atrophy',\n",
    "                  'Muscular Dystrophies',\n",
    "                  'Myasthenia Gravis',\n",
    "                  'Myoclonus',\n",
    "                  'Neuroaxonal Dystrophies',\n",
    "                  'Neurodegenerative Diseases',\n",
    "                  'Neurogenic Inflammation',\n",
    "                  'Neuromuscular Diseases',\n",
    "                  'Neuronal Ceroid-Lipofuscinoses',\n",
    "                  'Neuropathic Pain',\n",
    "                  'Neuropil Disorders',\n",
    "                  'Neurosyphilis',\n",
    "                  'Normal Pressure Hydrocephalus',\n",
    "                  'Parkinson Disease',\n",
    "                  'Peripheral Nervous System Diseases',\n",
    "                  'Phantom Limb',\n",
    "                  'Poliomyelitis',\n",
    "                  'Post-Concussion Syndrome',\n",
    "                  'Prion Diseases',\n",
    "                  'Progressive Bulbar Palsy',\n",
    "                  'Progressive Supranuclear Palsy',\n",
    "                  'Reflex Sympathetic Dystrophy',\n",
    "                  'Restless Legs Syndrome',\n",
    "                  'Retinal Diseases',\n",
    "                  'Retinal Ganglion Cell Loss',\n",
    "                  'Schilder Disease',\n",
    "                  'Shy-Drager Syndrome',\n",
    "                  'Spinal Cord Diseases',\n",
    "                  'Spinal Cord Injuries',\n",
    "                  'Spinal Muscular Atrophies of Childhood',\n",
    "                  'Stiff-Person Syndrome',\n",
    "                  'Stroke',\n",
    "                  'Subarachnoid Hemorrhage',\n",
    "                  'Subdural Hematoma',\n",
    "                  'Syringomyelia',\n",
    "                  'Tay-Sachs Disease',\n",
    "                  'Tourette Syndrome' ]\n",
    "\n",
    "gpt_neuro = gpt_neuro_phys + gpt_neuro_path\n",
    "gpt_neuro_ser = pd.Series(gpt_neuro)\n",
    "\n",
    "gpt_neuro_ser = gpt_neuro_ser.drop_duplicates()\n",
    "print(gpt_neuro_ser.nunique())\n",
    "gpt_neuro_ser\n",
    "\n",
    "print('physiologic topic')\n",
    "#mother_df = mesh_large_df_sty\n",
    "mother_df = mesh_litvar_sty\n",
    "#get mesh\n",
    "chat_mesh_neuro_phys = pd.DataFrame(columns=mother_df.columns) #Empty DataFrame, same structure\n",
    "time1 = datetime.now()\n",
    "for i in gpt_neuro_phys:\n",
    "    meshy = mother_df[mother_df['Preferred Label'].str.contains(i).fillna(False)]\n",
    "    #chat_mesh_neuro = chat_mesh_neuro.append([meshy])\n",
    "    chat_mesh_neuro_phys = pd.concat([meshy,chat_mesh_neuro_phys])\n",
    "    #chatmesh_var = pd.concat([meshy, chatmesh_var.loc[:]])\n",
    "time2 = datetime.now()\n",
    "print(time2-time1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('physiologic topic')\n",
    "#mother_df = mesh_large_df_sty\n",
    "mother_df = mesh_litvar_sty\n",
    "#get mesh\n",
    "chat_mesh_neuro_phys = pd.DataFrame(columns=mother_df.columns) #Empty DataFrame, same structure\n",
    "time1 = datetime.now()\n",
    "for i in gpt_neuro_phys:\n",
    "    meshy = mother_df[mother_df['Preferred Label'].str.contains(i).fillna(False)]\n",
    "    #chat_mesh_neuro = chat_mesh_neuro.append([meshy])\n",
    "    chat_mesh_neuro_phys = pd.concat([meshy,chat_mesh_neuro_phys])\n",
    "    #chatmesh_var = pd.concat([meshy, chatmesh_var.loc[:]])\n",
    "time2 = datetime.now()\n",
    "print(time2-time1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('mesh number:',chat_mesh_neuro_phys['Preferred Label'].nunique())\n",
    "chat_mesh_neuro_phys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "neuro_phys on global, mesh: 824\n",
    "neuro_phys on litvar, mesh: 438"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_mesh_neuro_phys_grup = chat_mesh_neuro_phys.groupby('Semantic Types Label').describe()\n",
    "chat_mesh_neuro_phys_grup.columns = chat_mesh_neuro_phys_grup.columns.to_flat_index()\n",
    "chat_mesh_neuro_phys_sem = chat_mesh_neuro_phys_grup.index.to_list()\n",
    "chat_mesh_neuro_phys_grup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('pahologic topic')\n",
    "#get mesh\n",
    "chat_mesh_neuro_path = pd.DataFrame(columns=mesh_large_df_sty.columns) #Empty DataFrame, same structure\n",
    "time1 =datetime.now()\n",
    "for i in gpt_neuro_path:\n",
    "    meshy = mesh_large_df_sty[mesh_large_df_sty['Preferred Label'].str.contains(i).fillna(False)]\n",
    "    #chat_mesh_neuro = chat_mesh_neuro.append([meshy])\n",
    "    chat_mesh_neuro_path = pd.concat([meshy,chat_mesh_neuro_path])\n",
    "    #chatmesh_var = pd.concat([meshy, chatmesh_var.loc[:]])\n",
    "time2 =datetime.now()\n",
    "print(time2-time1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(chat_mesh_neuro_path['Preferred Label'].nunique())\n",
    "chat_mesh_neuro_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#analyze semantics\n",
    "chat_mesh_neuro_path_grup = chat_mesh_neuro_path.groupby('Semantic Types Label').describe()\n",
    "chat_mesh_neuro_path_sem = chat_mesh_neuro_path_grup.index.to_list()\n",
    "chat_mesh_neuro_path_grup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get sum of semantic types\n",
    "chat_mesh_sem_sum = chat_mesh_neuro_path_sem + chat_mesh_neuro_phys_sem\n",
    "chat_mesh_sem_sum_ser = pd.Series(chat_mesh_sem_sum)\n",
    "print(chat_mesh_sem_sum_ser.nunique())\n",
    "chat_mesh_sem_sum_ser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get mesh\n",
    "chat_mesh_neuro = pd.DataFrame(columns=mesh_large_df_sty.columns) #Empty DataFrame, same structure\n",
    "for i in gpt_neuro_ser:\n",
    "    meshy = mesh_large_df_sty[mesh_large_df_sty['Preferred Label'].str.contains(i).fillna(False)]\n",
    "    #chat_mesh_neuro = chat_mesh_neuro.append([meshy])\n",
    "    chat_mesh_neuro = pd.concat([meshy,chat_mesh_neuro])\n",
    "    #chatmesh_var = pd.concat([meshy, chatmesh_var.loc[:]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('mesh', chat_mesh_neuro['Preferred Label'].nunique())\n",
    "chat_mesh_neuro_sem = chat_mesh_neuro['Semantic Types Label'].drop_duplicates()\n",
    "print('sty',chat_mesh_neuro['Semantic Types Label'].nunique())\n",
    "chat_mesh_neuro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#analyze semantics\n",
    "chat_mesh_neuro.groupby('Semantic Types Label').describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_mesh_neuro['Preferred Label'].nunique()#.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GET ref mesh list\n",
    "if os.path.exists('ref-mesh-archive/ref_mesh_neuro.csv'):\n",
    "    print('pass')\n",
    "    pass\n",
    "else:\n",
    "    chat_mesh_neuro[['Preferred Label', 'Semantic Types Label']].to_csv('ref-mesh-archive/ref_mesh_neuro.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "# grupuby describe:\n",
    "chat_mesh_neuro_count = chat_mesh_neuro.groupby('Semantic Types Label').describe()\n",
    "old_col = str(chat_mesh_neuro_count.columns.to_list())\n",
    "new_col = old_col.replace(\"\\', \\'\", \"_\")\n",
    "new_col = new_col.replace(\"(\", \"\")\n",
    "new_col = new_col.replace(\")\", \"\")\n",
    "chat_mesh_neuro_count.columns = literal_eval(new_col)\n",
    "chat_mesh_neuro_count.sort_values('Preferred Label_count', inplace=True, ascending=False)\n",
    "chat_mesh_neuro_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Rip form complete ref mesh\n",
    "rip_list = ['Cell', 'Receptor', 'Hormone', 'Tissue', 'Body Part, Organ, or Organ Component', 'Congenital Abnormality', 'Anatomical Abnormality', 'Indicator, Reagent, or Diagnostic Aid']\n",
    "\n",
    "#new_ref_sty = ref_sty[ref_sty['Semantic Types Label'].isin(remove_value)]\n",
    "mask = chat_mesh_neuro['Semantic Types Label'].isin(rip_list)\n",
    "new_ref_sty_neuro = chat_mesh_neuro[-mask]\n",
    "new_ref_sty_neuro_sty = new_ref_sty_neuro['Semantic Types Label'].drop_duplicates()\n",
    "\n",
    "if os.path.exists('ref-mesh-archive/new_ref_sty_neuro.csv'):\n",
    "    pass\n",
    "else:\n",
    "    new_ref_sty_neuro.to_csv('ref-mesh-archive/new_ref_sty_neuro.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(new_ref_sty_neuro['Preferred Label'].nunique())\n",
    "chat_mesh_neuro_count = new_ref_sty_neuro.groupby('Semantic Types Label').describe()\n",
    "old_col = str(chat_mesh_neuro_count.columns.to_list())\n",
    "new_col = old_col.replace(\"\\', \\'\", \"_\")\n",
    "new_col = new_col.replace(\"(\", \"\")\n",
    "new_col = new_col.replace(\")\", \"\")\n",
    "chat_mesh_neuro_count.columns = literal_eval(new_col)\n",
    "chat_mesh_neuro_count.sort_values('Preferred Label_count', inplace=True, ascending=False)\n",
    "chat_mesh_neuro_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_ref_sty_neuro_sty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### nutri trial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ChatGPT list\n",
    "gpt_nutri = [\"Nutrition\",\n",
    "\"Diet\",\n",
    "\"Energy Metabolism\",\n",
    "\"Macronutrients\",\n",
    "\"Carbohydrates\",\n",
    "\"Lipids\",\n",
    "\"Micronutrients\",\n",
    "\"Vitamins\",\n",
    "\"Minerals\",\n",
    "\"Trace Elements\",\n",
    "\"Antioxidants\",\n",
    "\"Nutrition Disorders\",\n",
    "\"Malnutrition\",\n",
    "\"Obesity\",\n",
    "\"Overweight\",\n",
    "\"Energy Balance\",\n",
    "\"Body Composition\",\n",
    "\"Body Mass Index\",\n",
    "\"Adiposity\",\n",
    "\"Nutrient Absorption\",\n",
    "\"Nutrient Transport\",\n",
    "\"Nutrient Metabolism\",\n",
    "\"Nutrient Requirements\",\n",
    "\"Nutritional Status\",\n",
    "\"Nutritional Assessment\",\n",
    "\"Nutritional Surveillance\",\n",
    "\"Nutritional Epidemiology\",\n",
    "\"Nutritional Physiology\",\n",
    "\"Nutrient Interactions\",\n",
    "\"Nutrient Toxicity\",\n",
    "\"Nutrient Deficiency\",\n",
    "\"Food Intake\",\n",
    "\"Appetite\",\n",
    "\"Food Preferences\",\n",
    "\"Feeding Behavior\",\n",
    "\"Food Habits\",\n",
    "\"Food Choice\",\n",
    "\"Eating Disorders\",\n",
    "\"Nutritional Genomics\",\n",
    "\"Nutrigenetics\",\n",
    "\"Nutrigenomics\",\n",
    "\"Epigenetics\",\n",
    "\"Genetic Variation\",\n",
    "\"Genotype\",\n",
    "\"Phenotype\",\n",
    "\"Gene Expression\",\n",
    "\"Single Nucleotide Polymorphisms\",\n",
    "\"MicroRNAs\",\n",
    "\"Enzymes\",\n",
    "\"Hormones\",\n",
    "\"Insulin\",\n",
    "\"Glucagon\",\n",
    "\"Leptin\",\n",
    "\"Adiponectin\",\n",
    "\"Ghrelin\",\n",
    "\"Cortisol\",\n",
    "\"Thyroid Hormones\",\n",
    "\"Growth Hormones\",\n",
    "\"Inflammation\",\n",
    "\"Oxidative Stress\",\n",
    "\"Immune System\",\n",
    "\"Aging\",\n",
    "\"Development\",\n",
    "\"Reproduction\",\n",
    "\"Maternal Nutrition\",\n",
    "\"Lactation\",\n",
    "\"Infant Nutrition\",\n",
    "\"Childhood Nutrition\",\n",
    "\"Adolescent Nutrition\",\n",
    "\"Elderly Nutrition\",\n",
    "\"Sports Nutrition\",\n",
    "\"Physical Activity\",\n",
    "\"Exercise\",\n",
    "\"Endurance\",\n",
    "\"Strength\",\n",
    "\"Bodybuilding\",\n",
    "\"Resistance Training\",\n",
    "\"Nutrition Supplementation\",\n",
    "\"Nutrient Timing\",\n",
    "\"Diet Supplements\",\n",
    "\"Ergogenic Aids\",\n",
    "\"Weight Management\",\n",
    "\"Low Carbohydrate Diets\",\n",
    "\"High Protein Diets\",\n",
    "\"Low Fat Diets\",\n",
    "\"Vegetarian Diets\",\n",
    "\"Vegan Diets\",\n",
    "\"Gluten-Free Diets\",\n",
    "\"Dairy-Free Diets\",\n",
    "\"Soy-Free Diets\",\n",
    "\"Detox Diets\",\n",
    "\"Intermittent Fasting\",\n",
    "\"Time-Restricted Feeding\",\n",
    "\"Meal Replacements\",\n",
    "\"Energy Bars\",\n",
    "\"Meal Supplements\",\n",
    "\"Hydration\",\n",
    "\"Water Requirements\",\n",
    "\"Nutrient Density\",\n",
    "\"Food Composition\",\n",
    "\"Food Processing\",\n",
    "\"Food Preservation\",\n",
    "\"Food Safety\",\n",
    "\"Food Additives\",\n",
    "\"Food Contaminants\",\n",
    "\"Food Fortification\",\n",
    "\"Food Supplementation\",\n",
    "\"Food Sources\",\n",
    "\"Food Availability\",\n",
    "\"Food Access\",\n",
    "\"Food Insecurity\",\n",
    "\"Food Assistance Programs\",\n",
    "\"Food Labeling\",\n",
    "\"Nutrient Recommendations\",\n",
    "\"Dietary Reference Intakes\",\n",
    "\"Recommended Dietary Allowances\",\n",
    "\"Adequate Intake\",\n",
    "\"Tolerable Upper Intake Level\",\n",
    "\"Nutrient Recommendations for Special Populations\",\n",
    "\"Pregnancy\",\n",
    "\"Infancy\",\n",
    "\"Childhood\",\n",
    "\"Adolescence\",\n",
    "\"Elderly\",\n",
    "\"Athletes\",\n",
    "\"Chronic Diseases\",\n",
    "\"Cardiovascular Disease\",\n",
    "\"Hypertension\",\n",
    "\"Diabetes\",\n",
    "\"Metabolic Syndrome\",\n",
    "\"Cancer\",\n",
    "\"Gastrointestinal Diseases\",\n",
    "\"Inflammatory Bowel Disease\",\n",
    "\"Non-alcoholic Fatty Liver Disease\",\n",
    "\"Nutrition Support\",\n",
    "\"Enteral Nutrition\",\n",
    "\"Parenteral Nutrition\",\n",
    "\"Artificial Feeding\",\n",
    "\"Nutritional Requirements in Critical Illness\",\n",
    "\"Nutritional Support in Surgery\",\n",
    "\"Nutritional Management of Chronic Wounds\",\n",
    "\"Nutritional Interventions\",\n",
    "\"Nutrition Education\",\n",
    "\"Food Skills Training\",\n",
    "\"Cooking Classes\",\n",
    "\"Meal Planning\",\n",
    "\"Food Budgeting\",\n",
    "\"Food Shopping\",\n",
    "\"Food Store Environment\",\n",
    "\"Food Marketing\",\n",
    "\"Food Advertising\",\n",
    "\"Food Industry\",\n",
    "\"Food Systems\",\n",
    "\"Agricultural Systems\",\n",
    "\"Livestock Production\",\n",
    "\"Seafood Production\",\n",
    "\"Food Trade\",\n",
    "\"Food Policy\",\n",
    "\"Food Security\",\n",
    "\"Food Sovereignty\",\n",
    "\"Food Justice\",\n",
    "\"Food System Sustainability\",\n",
    "\"Food Waste\",\n",
    "\"Sustainable Diets\",\n",
    "\"Plant-based Diets\",\n",
    "\"Mediterranean Diet\",\n",
    "\"Paleolithic Diet\",\n",
    "\"Traditional Diets\",\n",
    "\"Fermented Foods\",\n",
    "\"Probiotics\",\n",
    "\"Prebiotics\",\n",
    "\"Gut Microbiota\",\n",
    "\"Gut Health\",\n",
    "\"Gut-Brain Axis\",\n",
    "\"Neural Control of Feeding\",\n",
    "\"Brain-Derived Neurotrophic Factor\",\n",
    "\"Cholecystokinin\",\n",
    "\"Peptide YY\",\n",
    "\"Amylin\",\n",
    "\"Glucagon-Like Peptide 1\",\n",
    "\"Oxyntomodulin\",\n",
    "\"PYY3-36\",\n",
    "\"Agouti-Related Peptide\",\n",
    "\"Neuropeptide Y\",\n",
    "\"Cocaine- and Amphetamine-Regulated Transcript\",\n",
    "\"Melanin-Concentrating Hormone\",\n",
    "\"Orexins\",\n",
    "\"Pro-opiomelanocortin\",\n",
    "\"Corticotropin-Releasing Hormone\",\n",
    "\"Stress\",\n",
    "\"Sleep\",\n",
    "\"Circadian Rhythms\",\n",
    "\"Light-Dark Cycles\",\n",
    "\"Social Environment\",\n",
    "\"Culture\",\n",
    "\"Food Traditions\",\n",
    "\"Food Beliefs and Practices\",\n",
    "\"Carbohydrate Metabolism\",\n",
    "\"Lipid Metabolism\",\n",
    "\"Protein Metabolism\",\n",
    "\"Nitrogen Balance\",\n",
    "\"Macronutrient Metabolism\",\n",
    "\"Micronutrient Metabolism\",\n",
    "\"Vitamin Metabolism\",\n",
    "\"Mineral Metabolism\",\n",
    "\"Trace Element Metabolism\",\n",
    "\"Antioxidant Status\",\n",
    "\"Gut-Liver Axis\",\n",
    "\"Adipose Tissue\",\n",
    "\"Body Weight\",\n",
    "\"Waist Circumference\",\n",
    "\"Body Fat Distribution\",\n",
    "\"Resistin\",\n",
    "\"Insulin Resistance\",\n",
    "\"Insulin Sensitivity\",\n",
    "\"Insulin Secretion\",\n",
    "\"Glucose Homeostasis\",\n",
    "\"Hyperglycemia\",\n",
    "\"Hypoglycemia\",\n",
    "\"Type 2 Diabetes\",\n",
    "\"Impaired Glucose Tolerance\",\n",
    "\"Impaired Fasting Glucose\",\n",
    "\"Dyslipidemia\",\n",
    "\"Hypercholesterolemia\",\n",
    "\"Hypertriglyceridemia\",\n",
    "\"Low-Density Lipoprotein\",\n",
    "\"High-Density Lipoprotein\",\n",
    "\"Lipoprotein(a)\",\n",
    "\"Atherosclerosis\",\n",
    "\"Kidney Disease\",\n",
    "\"Renal Failure\",\n",
    "\"Renal Insufficiency\",\n",
    "\"Chronic Kidney Disease\",\n",
    "\"Endocrine Disorders\",\n",
    "\"Parathyroid Hormones\",\n",
    "\"Adrenal Hormones\",\n",
    "\"Pancreatic Hormones\",\n",
    "\"Reproductive Hormones\",\n",
    "\"Menopause\",\n",
    "\"Andropause\",\n",
    "\"Bone Health\",\n",
    "\"Osteoporosis\",\n",
    "\"Fractures\",\n",
    "\"Bone Mineral Density\",\n",
    "\"Bone Turnover\",\n",
    "\"Calcium Homeostasis\",\n",
    "\"Vitamin D Metabolism\",\n",
    "\"Muscle Mass\",\n",
    "\"Sarcopenia\",\n",
    "\"Muscle Strength\",\n",
    "\"Physical Performance\",\n",
    "\"Physical Function\",\n",
    "\"Physical Disability\",\n",
    "\"Endurance Training\",\n",
    "\"High-Intensity Interval Training\",\n",
    "\"Physical Inactivity\",\n",
    "\"Sedentary Behavior\",\n",
    "\"Geriatrics\",\n",
    "\"Age-Related Changes\",\n",
    "\"Cognitive Function\",\n",
    "\"Dementia\",\n",
    "\"Alzheimer's Disease\",\n",
    "\"Parkinson's Disease\",\n",
    "\"Huntington's Disease\",\n",
    "\"Multiple Sclerosis\",\n",
    "\"Neurodegeneration\",\n",
    "\"Nerve Function\",\n",
    "\"Neural Plasticity\",\n",
    "\"Sleep Disorders\",\n",
    "\"Mood\",\n",
    "\"Anxiety\",\n",
    "\"Depression\",\n",
    "\"Psychological Well-Being\",\n",
    "\"Quality of Life\",\n",
    "\"Life Expectancy\",\n",
    "\"Health Span\",\n",
    "\"Hunger\",\n",
    "\"Satiety\",\n",
    "\"Body Image\",\n",
    "\"Anorexia Nervosa\",\n",
    "\"Bulimia Nervosa\",\n",
    "\"Binge Eating Disorder\",\n",
    "\"Dietary Patterns\",\n",
    "\"Dietary Guidelines\",\n",
    "\"Nutrient Adequacy\",\n",
    "\"Nutrient Bioavailability\",\n",
    "\"Nutrient Storage\",\n",
    "\"Nutrient Utilization\",\n",
    "\"Food Preference\",\n",
    "\"Food Aversion\",\n",
    "\"Food Sensitivities\",\n",
    "\"Food Allergies\",\n",
    "\"Food Intolerance\",\n",
    "\"Food-Drug Interactions\",\n",
    "\"Nutrient Supplements\",\n",
    "\"Nutrient Fortification\",\n",
    "\"Nutrient Deficiencies\",\n",
    "\"Protein-Energy Malnutrition\",\n",
    "\"Kwashiorkor\",\n",
    "\"Marasmus\",\n",
    "\"Vitamin Deficiencies\",\n",
    "\"Vitamin A Deficiency\",\n",
    "\"Vitamin C Deficiency\",\n",
    "\"Vitamin D Deficiency\",\n",
    "\"Vitamin E Deficiency\",\n",
    "\"Vitamin K Deficiency\",\n",
    "\"Thiamin Deficiency\",\n",
    "\"Riboflavin Deficiency\",\n",
    "\"Niacin Deficiency\",\n",
    "\"Vitamin B6 Deficiency\",\n",
    "\"Vitamin B12 Deficiency\",\n",
    "\"Folate Deficiency\",\n",
    "\"Mineral Deficiencies\",\n",
    "\"Iron Deficiency\",\n",
    "\"Zinc Deficiency\",\n",
    "\"Calcium Deficiency\",\n",
    "\"Magnesium Deficiency\",\n",
    "\"Phosphorus Deficiency\",\n",
    "\"Potassium Deficiency\",\n",
    "\"Sodium Deficiency\",\n",
    "\"Iodine Deficiency\",\n",
    "\"Selenium Deficiency\",\n",
    "\"Chromium Deficiency\",\n",
    "\"Copper Deficiency\",\n",
    "\"Manganese Deficiency\",\n",
    "\"Molybdenum Deficiency\",\n",
    "\"Diet-Gene Interactions\",\n",
    "\"Genotype-Phenotype Interactions\",\n",
    "\"Genome-Wide Association Studies\",\n",
    "\"Personalized Nutrition\",\n",
    "\"Precision Nutrition\",\n",
    "\"Nutritional Status Assessment\",\n",
    "\"Dietary Assessment\",\n",
    "\"Biomarkers of Nutritional Status\",\n",
    "\"Serum Concentrations\",\n",
    "\"Plasma Concentrations\",\n",
    "\"Red Blood Cell Concentrations\",\n",
    "\"White Blood Cell Concentrations\",\n",
    "\"Hair Concentrations\",\n",
    "\"Nail Concentrations\",\n",
    "\"Saliva Concentrations\",\n",
    "\"Urine Concentrations\",\n",
    "\"Stool Concentrations\",\n",
    "\"Gastrointestinal Function\",\n",
    "\"Gut Microbiome\",\n",
    "\"Gut Permeability\",\n",
    "\"Irritable Bowel Syndrome\",\n",
    "\"Short-Chain Fatty Acids\",\n",
    "\"Synbiotics\",\n",
    "\"Fiber\",\n",
    "\"Resistant Starch\",\n",
    "\"Fermentable Carbohydrates\",\n",
    "\"Fermentation\",\n",
    "\"Digestion\",\n",
    "\"Fat Metabolism\",\n",
    "\"Nucleic Acid Metabolism\",\n",
    "\"Cholesterol Metabolism\",\n",
    "\"Triglycerides\",\n",
    "\"Fatty Acids\",\n",
    "\"Chylomicrons\",\n",
    "\"Lipoproteins\",\n",
    "\"Low-Density Lipoprotein Cholesterol\",\n",
    "\"High-Density Lipoprotein Cholesterol\",\n",
    "\"Very Low-Density Lipoprotein Cholesterol\",\n",
    "\"Intermediate-Density Lipoprotein Cholesterol\",\n",
    "\"Lipid Peroxidation\",\n",
    "\"Lipid Oxidation\",\n",
    "\"Free Radicals\",\n",
    "\"Reactive Oxygen Species\",\n",
    "\"Mitochondrial Function\",\n",
    "\"Glucose\",\n",
    "\"Glycogen\",\n",
    "\"Gluconeogenesis\",\n",
    "\"Type 2 Diabetes Mellitus\",\n",
    "\"Endocrine Pancreas\",\n",
    "\"Islets of Langerhans\",\n",
    "\"Beta Cells\",\n",
    "\"Alpha Cells\",\n",
    "\"Gastrointestinal Hormones\",\n",
    "\"Secretin\",\n",
    "\"Gastrin\",\n",
    "\"Somatostatin\",\n",
    "\"Glucagon-Like Peptide-1\",\n",
    "\"Interleukins\",\n",
    "\"Cytokines\",\n",
    "\"Tumor Necrosis Factor-Alpha\",\n",
    "\"Adipokines\",\n",
    "\"Hormone-Sensitive Lipase\",\n",
    "\"Peroxisome Proliferator-Activated Receptor-Gamma\",\n",
    "\"Peroxisome Proliferator-Activated Receptor-Alpha\",\n",
    "\"Nuclear Receptor Subfamily 1, Group H, Member 3\",\n",
    "\"Steroid Hormones\",\n",
    "\"Estrogens\",\n",
    "\"Androgens\",\n",
    "\"Progestogens\",\n",
    "\"Adrenal Cortex Hormones\",\n",
    "\"Adrenal Medulla Hormones\",\n",
    "\"Calcitonin\",\n",
    "\"Vitamin D\",\n",
    "\"Phosphorus\",\n",
    "\"Calcium\",\n",
    "\"Magnesium\",\n",
    "\"Bone Metabolism\",\n",
    "\"Bone Density\",\n",
    "\"Bone Mass\",\n",
    "\"Innate Immunity\",\n",
    "\"Adaptive Immunity\",\n",
    "\"Antibodies\",\n",
    "\"Complement System\",\n",
    "\"Phagocytosis\",\n",
    "\"Inflammatory Response\",\n",
    "\"Allergic Reactions\",\n",
    "\"Hypersensitivity Reactions\",\n",
    "\"Anaphylaxis\",\n",
    "\"Immune Tolerance\",\n",
    "\"Immune Suppression\",\n",
    "\"Autoimmunity\",\n",
    "\"Immune-Mediated Diseases\",\n",
    "\"Transplantation\",\n",
    "\"Graft Rejection\",\n",
    "\"T-Lymphocytes\",\n",
    "\"B-Lymphocytes\",\n",
    "\"Natural Killer Cells\",\n",
    "\"Monocytes\",\n",
    "\"Macrophages\",\n",
    "\"Dendritic Cells\",\n",
    "\"Lymphocytes\",\n",
    "\"Interferons\",\n",
    "\"Cytotoxic T-Lymphocytes\",\n",
    "\"Regulatory T-Lymphocytes\"]\n",
    "gpt_nutri_ser = pd.Series(gpt_nutri).drop_duplicates()\n",
    "\n",
    "#get mesh\n",
    "chat_mesh_nutri = pd.DataFrame(columns=mesh_large_df_sty.columns) #Empty DataFrame, same structure\n",
    "for i in gpt_nutri_ser:\n",
    "    meshy = mesh_large_df_sty[mesh_large_df_sty['Preferred Label'].str.contains(i).fillna(False)]\n",
    "    #chat_mesh_nutri = chat_mesh_nutri.append([meshy])\n",
    "    chat_mesh_nutri = pd.concat([meshy, chat_mesh_nutri])\n",
    "    #chatmesh_var = pd.concat([meshy, chatmesh_var.loc[:]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defined new_fer_mesh.csv for nutritional topic --> import it and model neuro un it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "new_nutri_refmesh = pd.read_csv('ref-mesh-archive/new_ref_mesh_large.csv', index_col=0)\n",
    "new_nutri_refmesh_count = new_nutri_refmesh.groupby('Semantic Types Label').describe()\n",
    "new_nutri_refmesh['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grupuby describe:\n",
    "new_nutri_refmesh = pd.read_csv('ref-mesh-archive/new_ref_mesh_large_corrected.csv', index_col=0)\n",
    "new_nutri_refmesh_sty = new_nutri_refmesh['Semantic Types Label'].drop_duplicates()\n",
    "\n",
    "new_nutri_refmesh_count = new_nutri_refmesh.groupby('Semantic Types Label').describe()\n",
    "old_col = str(new_nutri_refmesh_count.columns.to_list())\n",
    "new_col = old_col.replace(\"\\', \\'\", \"_\")\n",
    "new_col = new_col.replace(\"(\", \"\")\n",
    "new_col = new_col.replace(\")\", \"\")\n",
    "new_nutri_refmesh_count.columns = literal_eval(new_col)\n",
    "new_nutri_refmesh_count.sort_values('Preferred Label_count', inplace=True, ascending=False)\n",
    "new_nutri_refmesh_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_nutri_refmesh['Preferred Label'].nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#evalue STY overlapping\n",
    "neuro_nutri_sty_overrlap  = new_ref_sty_neuro_sty[new_ref_sty_neuro_sty.isin(new_nutri_refmesh_sty)]\n",
    "print( new_ref_sty_neuro_sty.nunique())\n",
    "print( new_nutri_refmesh_sty.nunique())\n",
    "print(neuro_nutri_sty_overrlap.nunique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Random Mesh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random grpm mesh list from MESH-STY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    list1 = [1, 2, 3, 4]\n",
    "    list2 = [3, 4, 5, 6]\n",
    "    list3 = [4, 5, 6, 7]\n",
    "\n",
    "    lists = [list1, list2, list3]\n",
    "    num_lists = len(lists)\n",
    "\n",
    "    # Initialize a 2D list of zeros with dimensions equal to the number of lists\n",
    "    cooccur_matrix = [[0] * num_lists for i in range(num_lists)]\n",
    "\n",
    "    # Loop over all pairs of lists and count the number of co-occurring elements\n",
    "    for i in range(num_lists):\n",
    "        for j in range(num_lists):\n",
    "            if i == j:\n",
    "                cooccur_matrix[i][j] = len(lists[i])\n",
    "            else:\n",
    "                cooccur_matrix[i][j] = len(set(lists[i]) & set(lists[j]))\n",
    "\n",
    "    # Print the resulting matrix with row and column headers\n",
    "    print(', '.join([''] + ['list{}'.format(i+1) for i in range(num_lists)]))\n",
    "    for i in range(num_lists):\n",
    "        row = ['list{}'.format(i+1)]\n",
    "        row.extend(cooccur_matrix[i])\n",
    "        print(', '.join(str(x) for x in row))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mesh_large_df_sty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate Random mesh list from complete df\n",
    "print(mesh_large_df_sty['Preferred Label'].nunique())\n",
    "mesh_large_df_sty_mesh = mesh_large_df_sty['Preferred Label'].drop_duplicates()\n",
    "\n",
    "size = 603\n",
    "sample_01 = mesh_large_df_sty_mesh.sample(n=size, random_state = 78954)\n",
    "sample_02 = mesh_large_df_sty_mesh.sample(n=size, random_state = 12245)\n",
    "sample_03 = mesh_large_df_sty_mesh.sample(n=size, random_state = 87498)\n",
    "sample_04 = mesh_large_df_sty_mesh.sample(n=size, random_state = 56798)\n",
    "sample_05 = mesh_large_df_sty_mesh.sample(n=size, random_state = 34565)\n",
    "sample_06 = mesh_large_df_sty_mesh.sample(n=size, random_state = 76523)\n",
    "sample_07 = mesh_large_df_sty_mesh.sample(n=size, random_state = 78968)\n",
    "sample_08 = mesh_large_df_sty_mesh.sample(n=size, random_state = 56845)\n",
    "sample_09 = mesh_large_df_sty_mesh.sample(n=size, random_state = 76624)\n",
    "sample_10 = mesh_large_df_sty_mesh.sample(n=size, random_state = 23845)\n",
    "\n",
    "sample_01.to_csv('ref-mesh-archive/random_01.csv')\n",
    "sample_02.to_csv('ref-mesh-archive/random_02.csv')\n",
    "sample_03.to_csv('ref-mesh-archive/random_03.csv')\n",
    "sample_04.to_csv('ref-mesh-archive/random_04.csv')\n",
    "sample_05.to_csv('ref-mesh-archive/random_05.csv')\n",
    "sample_06.to_csv('ref-mesh-archive/random_06.csv')\n",
    "sample_07.to_csv('ref-mesh-archive/random_07.csv')\n",
    "sample_08.to_csv('ref-mesh-archive/random_08.csv')\n",
    "sample_09.to_csv('ref-mesh-archive/random_09.csv')\n",
    "sample_10.to_csv('ref-mesh-archive/random_10.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random grpm mesh list from MESH-STY-LITVAR\n",
    "Generate Random Mesh list from grpm df\n",
    "\n",
    "    grpm_db_mesh = pd.read_csv('ref-mesh-archive/grpm_db_mesh.csv')\n",
    "    grpm_db_mesh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "if os.path.exists('ref-mesh-archive/grpm_db_mesh.csv'):\n",
    "    grpm_db_mesh = pd.read_csv('ref-mesh-archive/grpm_db_mesh.csv',index_col=0)\n",
    "    print('grpm mesh imported from archive')\n",
    "else:\n",
    "    grpmdb_table = pd.read_csv(r'H:\\Il mio Drive\\GRPM db\\GRPM db - Vitam\\grpm_db_pcg\\grpm_table_output.csv')\n",
    "    grpm_db_mesh = pd.DataFrame(grpmdb_table.mesh.drop_duplicates())\n",
    "    type(grpm_db_mesh)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "rand = pd.read_csv('ref-mesh-archive/random_grpm_03.csv')\n",
    "mask= grpm_db_mesh.mesh.isin(rand['mesh'])\n",
    "grpm_db_mesh[mask]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# randomize 10 samples\n",
    "size = 450\n",
    "sample_grpm_01 = grpm_db_mesh.mesh.sample(n=size, random_state = 78954)\n",
    "sample_grpm_02 = grpm_db_mesh.mesh.sample(n=size, random_state = 12245)\n",
    "sample_grpm_03 = grpm_db_mesh.mesh.sample(n=size, random_state = 87498)\n",
    "sample_grpm_04 = grpm_db_mesh.mesh.sample(n=size, random_state = 56798)\n",
    "sample_grpm_05 = grpm_db_mesh.mesh.sample(n=size, random_state = 34565)\n",
    "sample_grpm_06 = grpm_db_mesh.mesh.sample(n=size, random_state = 76523)\n",
    "sample_grpm_07 = grpm_db_mesh.mesh.sample(n=size, random_state = 78968)\n",
    "sample_grpm_08 = grpm_db_mesh.mesh.sample(n=size, random_state = 56845)\n",
    "sample_grpm_09 = grpm_db_mesh.mesh.sample(n=size, random_state = 76624)\n",
    "sample_grpm_00 = grpm_db_mesh.mesh.sample(n=size, random_state = 23845)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check fidelity\n",
    "mask= grpm_db_mesh.mesh.isin(sample_grpm_01)\n",
    "grpm_db_mesh[mask]\n",
    "#type(sample_grpm_10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save random samples\n",
    "if os.path.exists('ref-mesh-archive/random_grpm_00.csv'):\n",
    "    sample_grpm_01 = pd.read_csv('ref-mesh-archive/random_grpm_01.csv',index_col=0)\n",
    "    sample_grpm_02 = pd.read_csv('ref-mesh-archive/random_grpm_02.csv',index_col=0)\n",
    "    sample_grpm_03 = pd.read_csv('ref-mesh-archive/random_grpm_03.csv',index_col=0)\n",
    "    sample_grpm_04 = pd.read_csv('ref-mesh-archive/random_grpm_04.csv',index_col=0)\n",
    "    sample_grpm_05 = pd.read_csv('ref-mesh-archive/random_grpm_05.csv',index_col=0)\n",
    "    sample_grpm_06 = pd.read_csv('ref-mesh-archive/random_grpm_06.csv',index_col=0)\n",
    "    sample_grpm_07 = pd.read_csv('ref-mesh-archive/random_grpm_07.csv',index_col=0)\n",
    "    sample_grpm_08 = pd.read_csv('ref-mesh-archive/random_grpm_08.csv',index_col=0)\n",
    "    sample_grpm_09 = pd.read_csv('ref-mesh-archive/random_grpm_09.csv',index_col=0)\n",
    "    sample_grpm_00 = pd.read_csv('ref-mesh-archive/random_grpm_00.csv',index_col=0)\n",
    "\n",
    "save_rand = False\n",
    "if save_rand == True:\n",
    "    sample_grpm_01.to_csv('ref-mesh-archive/random_grpm_01.csv')\n",
    "    sample_grpm_02.to_csv('ref-mesh-archive/random_grpm_02.csv')\n",
    "    sample_grpm_03.to_csv('ref-mesh-archive/random_grpm_03.csv')\n",
    "    sample_grpm_04.to_csv('ref-mesh-archive/random_grpm_04.csv')\n",
    "    sample_grpm_05.to_csv('ref-mesh-archive/random_grpm_05.csv')\n",
    "    sample_grpm_06.to_csv('ref-mesh-archive/random_grpm_06.csv')\n",
    "    sample_grpm_07.to_csv('ref-mesh-archive/random_grpm_07.csv')\n",
    "    sample_grpm_08.to_csv('ref-mesh-archive/random_grpm_08.csv')\n",
    "    sample_grpm_09.to_csv('ref-mesh-archive/random_grpm_09.csv')\n",
    "    sample_grpm_00.to_csv('ref-mesh-archive/random_grpm_00.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(sample_grpm_01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_grpm_01_list = sample_grpm_01.to_list()\n",
    "sample_grpm_02_list = sample_grpm_02.to_list()\n",
    "sample_grpm_03_list = sample_grpm_03.to_list()\n",
    "sample_grpm_04_list = sample_grpm_04.to_list()\n",
    "sample_grpm_05_list = sample_grpm_05.to_list()\n",
    "sample_grpm_06_list = sample_grpm_06.to_list()\n",
    "sample_grpm_07_list = sample_grpm_07.to_list()\n",
    "sample_grpm_08_list = sample_grpm_08.to_list()\n",
    "sample_grpm_09_list = sample_grpm_09.to_list()\n",
    "sample_grpm_00_list = sample_grpm_00.to_list()\n",
    "\n",
    "lists= [sample_grpm_01_list,\n",
    "        sample_grpm_02_list,\n",
    "        sample_grpm_03_list,\n",
    "        sample_grpm_04_list,\n",
    "        sample_grpm_05_list,\n",
    "        sample_grpm_06_list,\n",
    "        sample_grpm_07_list,\n",
    "        sample_grpm_08_list,\n",
    "        sample_grpm_09_list,\n",
    "        sample_grpm_00_list]\n",
    "\n",
    "num_lists = len(lists)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coocuurance method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#(1) Initialize a 2D list of zeros with dimensions equal to the number of lists\n",
    "cooccur_matrix = [[0] * num_lists for i in range(num_lists)]\n",
    "cooccur_matrix = np.zeros((len(lists), len(lists)), dtype=int)\n",
    "\n",
    "type(cooccur_matrix)\n",
    "print(len(set(lists[1]) & set(lists[1])))\n",
    "print(len(set(lists[1]) & set(lists[3])))\n",
    "\n",
    "# Loop over all pairs of lists and count the number of co-occurring elements\n",
    "for i in range(num_lists):\n",
    "    for j in range(num_lists):\n",
    "        if i == j:\n",
    "            cooccur_matrix[i][j] = len(set(lists[i]))\n",
    "        else:\n",
    "            cooccur_matrix[i][j] = len(set(lists[i]) & set(lists[j])) # use set() to extract unique elements\n",
    "\n",
    "print(type(cooccur_matrix))\n",
    "\n",
    "# Convert the 2D list to a Pandas DataFrame\n",
    "cooccur_df = pd.DataFrame(cooccur_matrix,\n",
    "                          columns=['random{}'.format(i+1) for i in range(num_lists)],\n",
    "                          index=['random{}'.format(i+1) for i in range(num_lists)])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "cooccur_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Random grpm mesh list without rep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# COOCCURANCE MATRIX MODULE------------\n",
    "# second matrix build check\n",
    "sample_grpm_01_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep1.csv')\n",
    "sample_grpm_02_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep2.csv')\n",
    "sample_grpm_03_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep3.csv')\n",
    "sample_grpm_04_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep4.csv')\n",
    "sample_grpm_05_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep5.csv')\n",
    "sample_grpm_06_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep6.csv')\n",
    "sample_grpm_07_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep7.csv')\n",
    "sample_grpm_08_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep8.csv')\n",
    "sample_grpm_09_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep9.csv')\n",
    "sample_grpm_00_norep = pd.read_csv('ref-mesh-archive/random_grpm_mesh_norep/random_grpm_norep10.csv')\n",
    "\n",
    "lists= [sample_grpm_01_norep.mesh.to_list(),\n",
    "        sample_grpm_02_norep.mesh.to_list(),\n",
    "        sample_grpm_03_norep.mesh.to_list(),\n",
    "        sample_grpm_04_norep.mesh.to_list(),\n",
    "        sample_grpm_05_norep.mesh.to_list(),\n",
    "        sample_grpm_06_norep.mesh.to_list(),\n",
    "        sample_grpm_07_norep.mesh.to_list(),\n",
    "        sample_grpm_08_norep.mesh.to_list(),\n",
    "        sample_grpm_09_norep.mesh.to_list(),\n",
    "        sample_grpm_00_norep.mesh.to_list()]\n",
    "\n",
    "num_lists = len(lists)\n",
    "\n",
    "# Initialize a 2D list of zeros with dimensions equal to the number of lists\n",
    "cooccur_matrix = [[0] * num_lists for i in range(num_lists)]\n",
    "type(cooccur_matrix)\n",
    "print(len(set(lists[1]) & set(lists[1])))\n",
    "print(len(set(lists[1]) & set(lists[3])))\n",
    "\n",
    "# Loop over all pairs of lists and count the number of co-occurring elements\n",
    "for i in range(num_lists):\n",
    "    for j in range(num_lists):\n",
    "        if i == j:\n",
    "            cooccur_matrix[i][j] = len(lists[i])\n",
    "        else:\n",
    "            cooccur_matrix[i][j] = len(set(lists[i]) & set(lists[j]))\n",
    "\n",
    "# Convert the 2D list to a Pandas DataFrame\n",
    "cooccur_df = pd.DataFrame(cooccur_matrix, columns=['random{}'.format(i+1) for i in range(num_lists)], index=['random{}'.format(i+1) for i in range(num_lists)])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "cooccur_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the resulting matrix with row and column headers\n",
    "print(', '.join([''] + ['list{}'.format(i+1) for i in range(num_lists)]))\n",
    "\n",
    "for i in range(num_lists):\n",
    "    row = ['list{}'.format(i+1)]\n",
    "    row.extend(cooccur_matrix[i])\n",
    "    print(', '.join(str(x) for x in row))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "crea 2 set da 5 random mesh senza ripetizioni"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Random grpm mesh list without rep from MESH-STY-LITVAR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# randomize\n",
    "sample_grpm_full = grpm_db_mesh.mesh.sample(n=len(grpm_db_mesh), random_state = 782954)\n",
    "sample_grpm_full"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# chunk size\n",
    "size = 450\n",
    "# Split the DataFrame into smaller DataFrames of chunk length 450\n",
    "chunks = np.array_split(sample_grpm_full, len(sample_grpm_full) // 450 + 1)\n",
    "\n",
    "# Print the number of chunks and the length of each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk {}: {} rows'.format(i+1, len(chunk)))\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save each DataFrame chunk to a separate CSV file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv('ref-mesh-archive/random_grpm_norep{}.csv'.format(i+1), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try random mesh list based on STY proportions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define ref_sty\n",
    "ref_sty = pd.concat([new_ref_sty_neuro_sty, new_nutri_refmesh_sty]).drop_duplicates()\n",
    "strip  = ['Enzyme', 'Bacterium','Organism']\n",
    "strip2  = ['Disease or Syndrome']\n",
    "result = [item for item in new_nutri_refmesh_sty.to_list() if item not in strip]\n",
    "result2 = [item for item in result if item not in strip2]\n",
    "ref_sty_nutri = pd.Series(result)\n",
    "ref_sty_nutri_rest = pd.Series(result2)\n",
    "\n",
    "#mother df\n",
    "print('mother df',mesh_large_df_sty['Preferred Label'].nunique(), len(mesh_large_df_sty['Preferred Label']))\n",
    "\n",
    "#filter mother df with ref sty\n",
    "mesh_large_df_sty_subset = mesh_large_df_sty[mesh_large_df_sty['Semantic Types Label'].isin(ref_sty_nutri)]\n",
    "print('filtered df', mesh_large_df_sty_subset['Preferred Label'].nunique(), len(mesh_large_df_sty_subset['Preferred Label']))\n",
    "#mesh_large_df_sty_subset['Semantic Types Label'].drop_duplicates()\n",
    "mesh_large_df_sty_subset_rest = mesh_large_df_sty_subset[mesh_large_df_sty_subset['Semantic Types Label'].isin(ref_sty_nutri_rest)]\n",
    "\n",
    "mesh_large_df_sty_subset_rest\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mesh_large_df_sty_subset.groupby('Semantic Types Label').describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mesh_large_df_sty['Preferred Label'].nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get random mesh list from filtered df\n",
    "sample_rest = 603 - 255\n",
    "disease_sample = 255\n",
    "\n",
    "mesh_large_df_sty_subset_disease = mesh_large_df_sty_subset.loc[mesh_large_df_sty_subset['Semantic Types Label'] == 'Disease or Syndrome']\n",
    "random_mesh_disease_01 = mesh_large_df_sty_subset_disease.sample(n=disease_sample, random_state = 42)\n",
    "random_mesh_disease_02 = mesh_large_df_sty_subset_disease.sample(n=disease_sample, random_state = 124)\n",
    "random_mesh_01 = mesh_large_df_sty_subset.sample(n=sample_rest, random_state = 42)\n",
    "random_mesh_02 = mesh_large_df_sty_subset.sample(n=sample_rest, random_state = 124)\n",
    "\n",
    "random_mesh_01_con = pd.concat([random_mesh_disease_01, random_mesh_01])\n",
    "random_mesh_02_con = pd.concat([random_mesh_disease_02, random_mesh_02])\n",
    "random_mesh_02_con"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists('ref-mesh-archive/random_mesh_01_con.csv'):\n",
    "    pass\n",
    "else:\n",
    "    random_mesh_01_con.to_csv('ref-mesh-archive/random_mesh_01_con.csv')\n",
    "    random_mesh_02_con.to_csv('ref-mesh-archive/random_mesh_02_con.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_mesh_02_con.groupby('Semantic Types Label').describe()\n",
    "random_mesh_01_con.groupby('Semantic Types Label').describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disease_sample = 255\n",
    "amino_sample = 92\n",
    "biolog_sample = 65\n",
    "new_nutri_refmesh_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Reference Mesh list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "ref = pd.read_csv('ref-mesh-archive/ref_mesh_nest.csv', sep=\";\")\n",
    "print('nested ref mesh', ref['Class ID'].nunique())\n",
    "\n",
    "ref['Semantic Types'] = ref['Semantic Types'].apply(ast.literal_eval)\n",
    "#otherwise= dfg['col1'] = df['col1'].str.strip('[]').str.split(', ').apply(lambda x: [int(i) for i in x])\n",
    "sty = pd.read_csv('ref-mesh-archive/MeshSTY-code.csv',sep=';')\n",
    "sty = sty.rename(columns={'ID':'Semantic Types'})\n",
    "ref"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_large = []\n",
    "for i in range(len(ref)):\n",
    "    for sem in ref['Semantic Types'][i]: #dfrspost = mother table\n",
    "        out = ref['Preferred Label'][i],ref['Class ID'][i],sem,ref['Synonyms'][i],ref['Parents'][i],ref['CUI'][i],ref['AQL'][i],ref['TERMUI'][i]\n",
    "        ref_large.append(out)\n",
    "\n",
    "ref_large_df = pd.DataFrame(ref_large)\n",
    "new_col_names = ['Preferred Label','Class ID','Semantic Types','Synonyms','Parents','CUI','AQL','TERMUI']\n",
    "ref_large_df.columns = new_col_names\n",
    "ref_large_df\n",
    "#ref_large_df.to_csv('ref-mesh-archive/ref_mesh_large.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mesh semantics analyze on Ref list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## set df on ref mesh sty large\n",
    "\n",
    "if os.path.exists('ref-mesh-archive/ref_mesh_sty_large.csv'):\n",
    "    ref_sty = pd.read_csv('ref-mesh-archive/ref_mesh_sty_large.csv', index_col=0)\n",
    "else:\n",
    "    ref_sty = pd.merge(ref_large_df, sty, on='Semantic Types', how='inner').reset_index(drop=True)\n",
    "    #Add rsid coulmn con merge\n",
    "    #rspmidmesh_merge = pd.merge(pmidmesh, rsidpmid, on= 'pmids', how='inner').drop_duplicates().reindex(columns=['pmids', 'rsid', 'mesh'])\n",
    "    ref_sty = ref_sty.rename(columns={'Preferred Label_y':'Semantic Types Label','Preferred Label_x':'Preferred Label'})\n",
    "    #ref_sty = ref_sty.drop('Column1',axis=1)\n",
    "    #ref_sty.to_csv('ref-mesh-archive/ref_mesh_sty_large.csv')\n",
    "\n",
    "ref_sty = ref_sty[['Preferred Label', 'Semantic Types Label', 'Class ID', 'Semantic Types', 'Synonyms', 'Parents', 'CUI', 'AQL', 'TERMUI']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(ref_sty['Preferred Label'].nunique(), 'mesh total')\n",
    "ref_sty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#modulo groupby and bar\n",
    "ref_sty_less = ref_sty[['Preferred Label','Semantic Types Label']]\n",
    "\n",
    "### groupby.describe analysis by rsid--------------------\n",
    "ref_sty_less_count = ref_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "ref_sty_less_count.columns = ref_sty_less_count.columns.to_flat_index()\n",
    "new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "ref_sty_less_count.columns = new_column_names\n",
    "ref_sty_less_count_sort = ref_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "\n",
    "x = ref_sty_less_count_sort['Semantic Types Label'].iloc[:]\n",
    "y = ref_sty_less_count_sort['mesh-count'].iloc[:]\n",
    "plt.figure(figsize=(4, len(ref_sty_less_count_sort)*0.25))\n",
    "plt.title('Reference Mesh- Semantic Type enrichment', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "#plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "plt.ylabel('Semantic Types')\n",
    "plt.xlabel('mesh', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "# use log scale\n",
    "plt.gca().set_xscale('log')\n",
    "#plt.savefig('Reference Mesh- Semantic Type enrichment.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_sty_less_count_sort#['Semantic Types Label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# analizing abundance\n",
    "#count = [12, 21]\n",
    "#remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count'].isin(count)]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count']> 56]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "ref_sty[ref_sty['Semantic Types Label'].isin(remove_value)]['Semantic Types Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## STY Ripper form complete ref mesh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Rip form complete ref mesh\n",
    "save = False\n",
    "rip_list = ['Cell', 'Receptor', 'Hormone', 'Tissue', 'Body Part, Organ, or Organ Component', 'Congenital Abnormality', 'Anatomical Abnormality', 'Indicator, Reagent, or Diagnostic Aid']\n",
    "\n",
    "remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['Semantic Types Label'].isin(rip_list)]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count']> 56]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "\n",
    "#new_ref_sty = ref_sty[ref_sty['Semantic Types Label'].isin(remove_value)]\n",
    "mask = ref_sty['Semantic Types Label'].isin(rip_list)\n",
    "new_ref_sty = ref_sty[-mask]\n",
    "\n",
    "if save == True:\n",
    "    new_ref_sty.to_csv('ref-mesh-archive/new_ref_mesh_large_corrected.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_ref_sty['Preferred Label'].drop_duplicates().to_csv('ref-mesh-archive/new_ref_mesh_corrected.csv')\n",
    "new_ref_sty['Preferred Label'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Global Mesh df break through:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Search for all sons from a parent ID:\n",
    "    goal: merge all child mesh together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Search for all brothers from a parent ID:\n",
    "var4 = 'D000066888' #'Diet, Food, and Nutrition'\n",
    "var5 = 'D044623' #'Nutrition Therapy'\n",
    "var6 = 'D014808' #Nutritional and Metabolic Diseases\n",
    "var7 = 'D002318'#Cardiovascular Diseases\n",
    "var8 = 'D004066'#Digestive System Diseases\n",
    "var9 = 'D004700'#Endocrine System Diseases\n",
    "var10 = 'D006967'#Hypersensitivity\n",
    "\n",
    "sub = df[df['Parents'].str.contains(var4).fillna(False)]\n",
    "# use .dropna(inplace=True)   OR   df.fillna(0, inplace=True)\n",
    "sub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenare i risultati\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfgb = pd.DataFrame(columns=df.columns) #Empty DataFrame, same structure\n",
    "#dfgg = pd.DataFrame()\n",
    "dfgg = sub\n",
    "# DEVO ricercare in 'Parents' l'ID della categoria superiore\n",
    "\n",
    "#Parents Class ID list:\n",
    "kids = sub['Class ID'].tolist() #search for nested childs\n",
    "\n",
    "# Multiple search:\n",
    "for i in kids:\n",
    "    child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "    dfgg = dfgg.append([child])\n",
    "    dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "    kids = child['Class ID'].tolist()\n",
    "    for i in kids:\n",
    "        child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "        dfgg = dfgg.append([child])\n",
    "        #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "        kids = child['Class ID'].tolist()\n",
    "        for i in kids:\n",
    "            child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "            dfgg = dfgg.append([child])\n",
    "            #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "            kids = child['Class ID'].tolist()\n",
    "            for i in kids:\n",
    "                child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "                dfgg = dfgg.append([child])\n",
    "                #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "                kids = child['Class ID'].tolist()\n",
    "                for i in kids:\n",
    "                    child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "                    dfgg = dfgg.append([child])\n",
    "                    #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "                    kids = child['Class ID'].tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfgg.loc[:].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dfgb.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chat_mesh_nutri#.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "for i in data:\n",
    "    meshy = df[df['Synonyms'].str.contains(i).fillna(False)]\n",
    "    chatmesh = chatmesh.append([meshy])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#chatmesh.iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "chatmeshsub = chatmesh[['Preferred Label','Class ID','Synonyms']].drop_duplicates()\n",
    "chatmeshsub.to_csv('ref-mesh-archive/df5_chatmeshsub_unique-2312mesh.csv')\n",
    "chatmeshsub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(chatmeshsub2['Preferred Label'].drop_duplicates())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dfgb = pd.DataFrame(columns=df.columns) #Empty DataFrame, same structure\n",
    "#fgb = pd.concat([z, dfgb.loc[:]])\n",
    "#DEVO  ricercare in 'Parents' l'ID della categoria superiore\n",
    "dfgb\n",
    "#Multiple search:\n",
    "for i in lisid:\n",
    "    dfgb = pd.concat([i, dfgb.loc[:]])\n",
    "dfgb.loc[:]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "varl1 = ['D004035', 'D003922']\n",
    "#serch for class ID\n",
    "df[df['Class ID'].str.contains(var3)]\n",
    "\n",
    "dfgg = pd.DataFrame(columns=df.columns)\n",
    "for i in varl1:\n",
    "    #dfg.append(df[df['Class ID'].str.contains(i)], ignore_index=True)\n",
    "    row = df[df['Class ID'].str.contains(i)]\n",
    "    dbv= pd.concat([row, dfgg.loc[:]])\n",
    "dbv # dont' work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#Create new empty dataframe\n",
    "dfg = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "ds1 = df[df['Class ID'].str.contains(var3)]\n",
    "ds2 = df[df['Class ID'].str.contains(var4)]\n",
    "#dfg.append(arr, ignore_index=True)\n",
    "#dfg2 = dfg.append(ds1)\n",
    "dfg2 = pd.concat([ds2, dfg2.loc[:]])\n",
    "dfg2\n",
    "#USE CONCAT!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Importing pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Creating the first Dataframe using dictionary\n",
    "df1 = df = pd.DataFrame({\"a\":[1, 2, 3, 4],\n",
    "                         \"b\":[5, 6, 7, 8]})\n",
    "\n",
    "# Creating the Second Dataframe using dictionary\n",
    "df2 = pd.DataFrame({\"a\":[1, 2, 3],\n",
    "                    \"b\":[5, 6, 7]})\n",
    "\n",
    "# Print df1\n",
    "print(df1, \"\\n\")\n",
    "\n",
    "# Print df2\n",
    "print(df2)\n",
    "# to append df2 at the end of df1 dataframe\n",
    "df1.append(df2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dft = pd.DataFrame()\n",
    "# Create a new dataframe with the desired columns and values\n",
    "new_data = pd.DataFrame({'Name': ['John', 'Jane'], 'Age': [25, 30]})\n",
    "\n",
    "# Append the new dataframe to the existing one\n",
    "dft = dft.append(new_data, ignore_index=True)\n",
    "\n",
    "for i in varl1:\n",
    "    dft.append(i, ignore_index=True)\n",
    "dft"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "dd= df[['Preferred Label','MeSH Frequency']]\n",
    "ddf= dd.dropna(subset='Preferred Label')\n",
    "print(ddf.shape)\n",
    "print(dd.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter(dd['Preferred Label'], dd['MeSH Frequency'])\n",
    "ddf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df[\"Preferred Label\"].__contains__(\"human\")\n",
    "df2=df[df[\"Preferred Label\"].str.contains('Polymorphism')]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df3= df2[['Preferred Label','MeSH Frequency']]\n",
    "df3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2=df[df[\"Preferred Label\"].str.contains('Diabetes')]\n",
    "df3= df2[['Preferred Label','MeSH Frequency']]\n",
    "df3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2=df[df[\"Preferred Label\"].str.contains('Fabry')]\n",
    "df3= df2[['Preferred Label','MeSH Frequency']]\n",
    "df3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " #https://www.nlm.nih.gov/databases/download/data_distrib_main.html\n",
    " #https://www.nlm.nih.gov/databases/download/mesh.html\n",
    " #https://id.nlm.nih.gov/mesh/swagger/ui"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confronto di specifiche MESH list con il MESH big data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dn = pd.read_csv('ref-mesh-archive/nbdb1-mesh.csv')\n",
    "\n",
    "print(len(dn['mesh'].drop_duplicates()))\n",
    "dga= df[df['Preferred Label'].isin(dn['mesh'])]\n",
    "\n",
    "#dgb = dn[-df['Preferred Label'].isin(dn['mesh'])].drop_duplicates()\n",
    "\n",
    "#see not contained in all-MESH\n",
    "dgaa= dga['Preferred Label']\n",
    "dgb = dn['mesh'].drop_duplicates()\n",
    "dgg = dgb[dgb.isin(dgaa)==False]\n",
    "dgg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Interpolazione con mesh selected fprm semantic type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dn = pd.read_csv('ref-mesh-archive/MeshEX-meshsorted.csv',sep=';')\n",
    "print(len(dn['Preferred Label'].drop_duplicates()))\n",
    "dnmask = dn.Column2.isin(['green'])\n",
    "dnfilter = dn[dnmask]\n",
    "dnfilter\n",
    "# moldulo 'Allineator'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### moldulo 'Allineator' for list value\n",
    "dnallign = []\n",
    "for i in range(len(dnfilter)):\n",
    "    for pmid in dnfilter['Semantic Types'][i]: #dfrspost = mother table\n",
    "        #out = (dfrspost['rsid'][i], pmid)\n",
    "        out = dnfilter['Preferred Label'][i], pmid\n",
    "        dnallign.append(out)\n",
    "print(type(dnallign))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MODULO per table explosion:\n",
    "\n",
    "data.csv = mesh; 'Semantic Types'; ID\n",
    "Bone Density,\"T201,T081\",D015519\n",
    "Ideal Body Weight,\"T201,T074,T081\",D056865ù\n",
    "\n",
    "split the SEM column by comma and explode the values to create a new row for each SEM value\n",
    "df = df.assign(SEM=df['SEM'].str.split(',')).explode('SEM')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MOLDULO per table explosion:\n",
    "\n",
    "# split the SEM column by comma and explode the values to create a new row for each SEM value\n",
    "dnfilter = dnfilter.assign(SEM=dnfilter['Semantic Types'].str.split(',')).explode('SEM')\n",
    "\n",
    "# print the updated table\n",
    "dnfilerstimple = dnfilter[['Preferred Label','Class ID','Semantic Types','SEM']]\n",
    "dnfiltsupersimple = dnfilter[['Preferred Label','SEM']].drop_duplicates()\n",
    "dnfiltsupersimple.to_clipboard()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Modulo analisi groupby:\n",
    "dnfilercount= dnfiltsupersimple.groupby('SEM').describe().reset_index()\n",
    "dnfilercount.columns = dnfilercount.columns.to_flat_index()\n",
    "new_column_names = ['SEM', 'Labes_count', 'Labes_count_unique','Labes_top','Labels_freq']\n",
    "dnfilercount.columns = new_column_names\n",
    "dnfilercountsort = dnfilercount.sort_values(by= 'Labes_count' ,ascending = False)\n",
    "dnfilercountsortsmall= dnfilercountsort[['SEM','Labes_count']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add semantic name column #Use Merge\n",
    "semcode = pd.read_csv('ref-mesh-archive/MeshSTY-code.csv',sep=';')\n",
    "new_column_names = ['Label', 'SEM']\n",
    "semcode.columns = new_column_names\n",
    "dnfilercountsortsmall2 = pd.merge(dnfilercountsortsmall, semcode, on='SEM', how='inner')\n",
    "print(len(dnfilercountsortsmall.SEM))\n",
    "dnfilercountsortsmall2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = dnfilercountsortsmall2.iloc[:,2]\n",
    "y = dnfilercountsortsmall2.iloc[:,1]\n",
    "#plt.figure(figsize=(15, 12))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.title('Scatter Plot: \"nutritional physiology\" reference mesh plot')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "w = dnfilercountsortsmall2.Labes_count[dnfilercountsortsmall2['Labes_count'] >1]\n",
    "u = dnfilercountsortsmall2.Label[dnfilercountsortsmall2['Labes_count'] >1]\n",
    "\n",
    "#plt.figure(figsize=(15, 12))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(u,w)\n",
    "\n",
    "plt.title('Scatter Plot: \"nutritional physiology\" reference mesh plot')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()\n",
    "print(len(dnfilercountsortsmall2.Label[dnfilercountsortsmall2['Labes_count'] >1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dnfilercountsortsmall2[dnfilercountsortsmall2['Labes_count'] >1].head(8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dga= df[df['Preferred Label'].isin(dnfilter['Preferred Label'])]\n",
    "\n",
    "#dga['Preferred Label'].drop_duplicates()\n",
    "dga"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
